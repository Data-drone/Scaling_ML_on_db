{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Large-Scale Imbalanced Dataset for XGBoost Scaling\n",
    "\n",
    "Generates a synthetic imbalanced classification dataset:\n",
    "- **64,050,659 rows**\n",
    "- **500 features**\n",
    "- **~2% minority class**\n",
    "\n",
    "Uses PySpark for distributed generation. Writes to Unity Catalog Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "TOTAL_ROWS = 64_050_659\n",
    "N_FEATURES = 500\n",
    "N_INFORMATIVE = 80\n",
    "MINORITY_RATIO = 0.02\n",
    "SEED = 42\n",
    "\n",
    "# Output - Unity Catalog table\n",
    "CATALOG = \"your_catalog\"\n",
    "SCHEMA = \"your_schema\"\n",
    "TABLE_NAME = \"imbalanced_64m\"\n",
    "\n",
    "OUTPUT_TABLE = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, randn, when, col, lit\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "\n",
    "def generate_imbalanced_dataset(\n",
    "    total_rows: int,\n",
    "    n_features: int,\n",
    "    n_informative: int,\n",
    "    minority_ratio: float,\n",
    "    seed: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate large imbalanced classification dataset.\n",
    "    \n",
    "    - First n_informative features are correlated with target\n",
    "    - Remaining features are noise\n",
    "    \"\"\"\n",
    "    # Base dataframe\n",
    "    df = spark.range(0, total_rows)\n",
    "    \n",
    "    # Imbalanced label\n",
    "    df = df.withColumn(\n",
    "        \"label\",\n",
    "        when(rand(seed) < minority_ratio, lit(1)).otherwise(lit(0)).cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Add features\n",
    "    for i in range(n_features):\n",
    "        feature_seed = seed + i + 1\n",
    "        \n",
    "        if i < n_informative:\n",
    "            # Informative: correlated with label\n",
    "            weight = 0.5 + (i % 10) * 0.15\n",
    "            df = df.withColumn(\n",
    "                f\"f{i}\",\n",
    "                (randn(feature_seed) + col(\"label\") * lit(weight)).cast(FloatType())\n",
    "            )\n",
    "        else:\n",
    "            # Noise\n",
    "            df = df.withColumn(f\"f{i}\", randn(feature_seed).cast(FloatType()))\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Added {i + 1}/{n_features} features...\")\n",
    "    \n",
    "    # Reorder: features first, then label\n",
    "    feature_cols = [f\"f{i}\" for i in range(n_features)]\n",
    "    return df.select(feature_cols + [\"label\"])\n",
    "\n",
    "\n",
    "print(f\"Generating: {TOTAL_ROWS:,} rows x {N_FEATURES} features\")\n",
    "print(f\"Minority class: {MINORITY_RATIO:.1%}\")\n",
    "print()\n",
    "\n",
    "df = generate_imbalanced_dataset(\n",
    "    total_rows=TOTAL_ROWS,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=N_INFORMATIVE,\n",
    "    minority_ratio=MINORITY_RATIO,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Writing to: {OUTPUT_TABLE}\")\n",
    "\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.table(OUTPUT_TABLE)\n",
    "\n",
    "print(f\"Rows: {df_check.count():,}\")\n",
    "print(f\"Columns: {len(df_check.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "df_check.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sample\n",
    "sample_cols = [f\"f{i}\" for i in range(5)] + [\"label\"]\n",
    "df_check.select(sample_cols).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify informative vs noise features\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "print(\"Mean by label (informative f0 should differ, noise f400 should not):\")\n",
    "df_check.groupBy(\"label\").agg(\n",
    "    mean(\"f0\").alias(\"f0_mean\"),\n",
    "    mean(\"f400\").alias(\"f400_mean\")\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
