{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Imbalanced Dataset\n",
    "\n",
    "Generates synthetic imbalanced classification datasets for XGBoost scaling experiments.\n",
    "\n",
    "**Parameters** (via widgets or job params):\n",
    "- `env`: Environment name (dev/prod)\n",
    "- `run_mode`: `full` or `smoke` (smoke uses tiny data for quick validation)\n",
    "- `json_params`: JSON string with additional config overrides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget definitions - these can be overridden by job parameters\n",
    "dbutils.widgets.text(\"env\", \"dev\", \"Environment\")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"full\", [\"full\", \"smoke\"], \"Run Mode\")\n",
    "dbutils.widgets.text(\"json_params\", \"{}\", \"JSON Parameters\")\n",
    "\n",
    "# Catalog/schema widgets (can be set by job or bundle variables)\n",
    "dbutils.widgets.text(\"catalog\", \"brian_gen_ai\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"xgb_scaling\", \"Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Parse Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# Add src to path for local imports\n",
    "# When deployed via DAB, the repo files are synced to workspace\n",
    "import os\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_root = \"/\".join(notebook_path.split(\"/\")[:-2])  # Go up from /notebooks/notebook_name\n",
    "sys.path.insert(0, f\"/Workspace{repo_root}\")\n",
    "\n",
    "# Import core logic\n",
    "from src.main import run, build_exit_result\n",
    "from src.config import DatasetConfig\n",
    "\n",
    "# Get widget values\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "run_mode = dbutils.widgets.get(\"run_mode\")\n",
    "json_params = dbutils.widgets.get(\"json_params\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "# Parse parameters and get config\n",
    "config = run(\n",
    "    env=env,\n",
    "    run_mode=run_mode,\n",
    "    json_params=json_params,\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql.functions import (\n    rand, randn, when, col, lit, floor, abs as spark_abs,\n    concat_ws, pandas_udf, array, struct\n)\nfrom pyspark.sql.types import (\n    FloatType, IntegerType, StringType, StructType, StructField, ArrayType\n)\nimport pyspark.sql.functions as F\n\n# Categorical feature configurations — mix of different cardinalities\n# to simulate real-world data (e.g., country, status, category, etc.)\nCATEGORICAL_CONFIGS = [\n    {\"name\": \"binary\",       \"cardinality\": 2,    \"prefix\": \"cat_bin\",  \"weight\": 0.15},\n    {\"name\": \"low_card\",     \"cardinality\": 5,    \"prefix\": \"cat_low\",  \"weight\": 0.10},\n    {\"name\": \"medium_card\",  \"cardinality\": 20,   \"prefix\": \"cat_med\",  \"weight\": 0.08},\n    {\"name\": \"high_card\",    \"cardinality\": 100,  \"prefix\": \"cat_hi\",   \"weight\": 0.05},\n    {\"name\": \"very_high\",    \"cardinality\": 500,  \"prefix\": \"cat_vhi\",  \"weight\": 0.03},\n]\n\n\ndef generate_imbalanced_dataset(\n    spark,\n    total_rows: int,\n    n_features: int,\n    n_informative: int,\n    n_categorical: int,\n    minority_ratio: float,\n    seed: int,\n):\n    \"\"\"\n    Generate large imbalanced classification dataset using Spark.\n\n    OPTIMIZED for large datasets (100M+ rows):\n    - Uses batched select() instead of iterative withColumn() to avoid\n      massive Spark logical plans that cause OOM on the driver.\n    - Adds columns in batches of ~50, with explicit repartitioning to\n      keep partition sizes manageable.\n\n    Features:\n    - Numerical features: continuous floats (Gaussian distributed)\n      - First n_informative are correlated with the target label\n      - Remaining numerical features are random noise\n    - Categorical features: string columns with varying cardinalities\n      - Mix of binary (2), low (5), medium (20), high (100), very high (500)\n      - Some are correlated with the target (informative)\n    - Label: imbalanced binary (0/1) with minority_ratio as positive class proportion\n    \"\"\"\n    n_numerical = n_features - n_categorical\n\n    print(f\"Generating: {total_rows:,} rows x {n_features} features\")\n    print(f\"  Numerical features: {n_numerical}\")\n    print(f\"  Categorical features: {n_categorical}\")\n    print(f\"  Informative features: {n_informative}\")\n    print(f\"  Minority ratio: {minority_ratio:.1%}\")\n\n    # Choose partition count based on data size\n    # Target ~2M rows per partition for good parallelism\n    n_partitions = max(8, total_rows // 2_000_000)\n    print(f\"  Partitions: {n_partitions}\")\n    print()\n\n    # -------------------------------------------------------------------------\n    # Step 1: Base dataframe with id + label\n    # -------------------------------------------------------------------------\n    df = spark.range(0, total_rows, numPartitions=n_partitions)\n\n    # Imbalanced label (1 = minority class)\n    df = df.withColumn(\n        \"label\",\n        when(rand(seed) < minority_ratio, lit(1)).otherwise(lit(0)).cast(IntegerType())\n    )\n\n    # -------------------------------------------------------------------------\n    # Step 2: Generate ALL numerical features in one select() call\n    #         This avoids building a massive logical plan from 500+ withColumn() calls.\n    # -------------------------------------------------------------------------\n    BATCH_SIZE = 50  # Add columns in batches to keep plan size manageable\n\n    print(f\"Generating {n_numerical} numerical features (batches of {BATCH_SIZE})...\")\n    existing_cols = df.columns  # [\"id\", \"label\"]\n\n    for batch_start in range(0, n_numerical, BATCH_SIZE):\n        batch_end = min(batch_start + BATCH_SIZE, n_numerical)\n        new_cols = []\n\n        for i in range(batch_start, batch_end):\n            feature_seed = seed + i + 1\n            if i < n_informative:\n                # Informative: correlated with label\n                weight = 0.5 + (i % 10) * 0.15\n                new_cols.append(\n                    (randn(feature_seed) + col(\"label\") * lit(weight)).cast(FloatType()).alias(f\"f{i}\")\n                )\n            else:\n                # Noise: pure random\n                new_cols.append(\n                    randn(feature_seed).cast(FloatType()).alias(f\"f{i}\")\n                )\n\n        # Select existing + new batch in one operation\n        df = df.select(\"*\", *new_cols)\n\n        print(f\"  Batch {batch_start}-{batch_end - 1} done ({batch_end}/{n_numerical})\")\n\n        # Checkpoint periodically to cut the lineage and prevent driver OOM\n        if batch_end % 200 == 0 and batch_end < n_numerical:\n            print(f\"  [Checkpointing at {batch_end} features to cut lineage...]\")\n            df = df.localCheckpoint(eager=True)\n\n    # -------------------------------------------------------------------------\n    # Step 3: Generate ALL categorical features in one select() call\n    # -------------------------------------------------------------------------\n    if n_categorical > 0:\n        print(f\"\\nGenerating {n_categorical} categorical features (batches of {BATCH_SIZE})...\")\n\n        n_informative_cat = max(1, n_categorical // 5)  # ~20% are informative\n\n        for batch_start in range(0, n_categorical, BATCH_SIZE):\n            batch_end = min(batch_start + BATCH_SIZE, n_categorical)\n            new_cols = []\n\n            for cat_offset in range(batch_start, batch_end):\n                config_idx = cat_offset % len(CATEGORICAL_CONFIGS)\n                cfg = CATEGORICAL_CONFIGS[config_idx]\n                cardinality = cfg[\"cardinality\"]\n                prefix = cfg[\"prefix\"]\n                feature_seed = seed + n_numerical + cat_offset + 1000\n                col_name = f\"{prefix}_{cat_offset}\"\n\n                if cat_offset < n_informative_cat:\n                    # Informative categorical: distribution depends on label\n                    weight = cfg[\"weight\"]\n                    cat_int = (\n                        floor(\n                            spark_abs(randn(feature_seed) + col(\"label\") * lit(weight))\n                            * lit(cardinality / 3.0)\n                        ) % lit(cardinality)\n                    ).cast(IntegerType())\n                else:\n                    # Noise categorical: uniform random\n                    cat_int = floor(rand(feature_seed) * lit(cardinality)).cast(IntegerType())\n\n                # Convert to string label like \"cat_bin_0\", \"cat_low_3\"\n                new_cols.append(\n                    concat_ws(\"_\", lit(prefix), cat_int.cast(StringType())).alias(col_name)\n                )\n\n            # Select existing + new batch\n            df = df.select(\"*\", *new_cols)\n\n            print(f\"  Cat batch {batch_start}-{batch_end - 1} done ({batch_end}/{n_categorical})\")\n\n        print(f\"  Completed all {n_categorical} categorical features\")\n\n    # -------------------------------------------------------------------------\n    # Step 4: Final column ordering and cleanup\n    # -------------------------------------------------------------------------\n    # Reorder: numerical features, then categorical features, then label (drop id)\n    numerical_cols = [f\"f{i}\" for i in range(n_numerical)]\n    categorical_cols = []\n    for cat_idx in range(n_categorical):\n        cfg = CATEGORICAL_CONFIGS[cat_idx % len(CATEGORICAL_CONFIGS)]\n        categorical_cols.append(f\"{cfg['prefix']}_{cat_idx}\")\n\n    all_cols = numerical_cols + categorical_cols + [\"label\"]\n    df = df.select(all_cols)\n\n    # Final repartition for optimal Delta write parallelism\n    target_write_partitions = max(n_partitions, 200)\n    if total_rows >= 50_000_000:\n        target_write_partitions = max(n_partitions, 400)\n    print(f\"\\nRepartitioning to {target_write_partitions} partitions for Delta write...\")\n    df = df.repartition(target_write_partitions)\n\n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "start_time = time.time()\n",
    "\n",
    "df = generate_imbalanced_dataset(\n",
    "    spark=spark,\n",
    "    total_rows=config.total_rows,\n",
    "    n_features=config.n_features,\n",
    "    n_informative=config.n_informative,\n",
    "    n_categorical=config.n_categorical,\n",
    "    minority_ratio=config.minority_ratio,\n",
    "    seed=config.seed,\n",
    ")\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"\\nDataFrame created in {generation_time:.1f}s (lazy - not materialized yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Writing to: {config.output_table}\")\n",
    "\n",
    "write_start = time.time()\n",
    "\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(config.output_table)\n",
    "\n",
    "write_time = time.time() - write_start\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Write completed in {write_time:.1f}s\")\n",
    "print(f\"Total time: {total_time:.1f}s ({total_time / 60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back and validate\n",
    "df_check = spark.table(config.output_table)\n",
    "\n",
    "# Row count\n",
    "row_count = df_check.count()\n",
    "print(f\"Rows written: {row_count:,}\")\n",
    "print(f\"Expected:     {config.total_rows:,}\")\n",
    "print(f\"Match: {row_count == config.total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "label_counts = df_check.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "class_distribution = {}\n",
    "for row in label_counts:\n",
    "    label = row[\"label\"]\n",
    "    count = row[\"count\"]\n",
    "    pct = count / row_count * 100\n",
    "    class_name = \"Minority\" if label == 1 else \"Majority\"\n",
    "    print(f\"  Label {label} ({class_name}): {count:,} ({pct:.2f}%)\")\n",
    "    class_distribution[label] = count\n",
    "\n",
    "if len(label_counts) == 2:\n",
    "    print(f\"\\nImbalance ratio: {label_counts[0]['count'] / label_counts[1]['count']:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick sample — show mix of numerical + categorical + label\nn_numerical = config.n_features - config.n_categorical\nsample_num = [f\"f{i}\" for i in range(min(3, n_numerical))]\nsample_cat = []\ncat_idx = 0\ncfg_idx = 0\nwhile len(sample_cat) < min(3, config.n_categorical):\n    cfg = [\n        {\"prefix\": \"cat_bin\"}, {\"prefix\": \"cat_low\"}, {\"prefix\": \"cat_med\"},\n        {\"prefix\": \"cat_hi\"}, {\"prefix\": \"cat_vhi\"},\n    ]\n    sample_cat.append(f\"{cfg[cfg_idx % 5]['prefix']}_{cat_idx}\")\n    cat_idx += 1\n    cfg_idx += 1\n\nsample_cols = sample_num + sample_cat + [\"label\"]\nprint(f\"Sample data (first 5 rows, {len(sample_cols)} selected features + label):\")\ndf_check.select(sample_cols).show(5, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exit with Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build result for job output\n",
    "result_json = build_exit_result(\n",
    "    config=config,\n",
    "    status=\"ok\",\n",
    "    row_count=row_count,\n",
    "    duration_seconds=total_time,\n",
    "    class_distribution=class_distribution,\n",
    ")\n",
    "\n",
    "print(f\"\\nNotebook result:\")\n",
    "print(result_json)\n",
    "\n",
    "# Exit with JSON result (fetchable via Databricks API)\n",
    "dbutils.notebook.exit(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}