{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost (GPU Accelerated)\n",
    "\n",
    "GPU-accelerated XGBoost training using `tree_method=\"gpu_hist\"` with MLflow tracking.\n",
    "\n",
    "**Requirements:**\n",
    "- Databricks ML Runtime 17.3 LTS GPU (`17.3.x-gpu-ml-scala2.13`)\n",
    "- GPU-enabled cluster (NC-series Azure VMs)\n",
    "- Single GPU: NC4as_T4_v3 (1x T4, 16 GB)\n",
    "- Multi-GPU: NC16as_T4_v3 (4x T4, 64 GB total)\n",
    "\n",
    "**Parameters:**\n",
    "- `data_size`: Dataset size preset\n",
    "- `node_type`: GPU VM type (NC4asT4v3, NC16asT4v3)\n",
    "- `gpu_id`: GPU device ID (0 for single GPU)\n",
    "- `run_mode`: full or smoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"data_size\", \"tiny\", [\"tiny\", \"small\", \"medium\", \"medium_large\", \"large\", \"xlarge\"], \"Data Size\")\n",
    "dbutils.widgets.text(\"node_type\", \"NC4asT4v3\", \"Node Type\")\n",
    "dbutils.widgets.text(\"gpu_id\", \"0\", \"GPU ID\")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"full\", [\"full\", \"smoke\"], \"Run Mode\")\n",
    "dbutils.widgets.text(\"catalog\", \"brian_gen_ai\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"xgb_scaling\", \"Schema\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Table Name (override)\")  # Optional: override auto table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Error Tracking ---\n",
    "# Collects errors during execution so the exit cell can report them\n",
    "# even if training fails partway through.\n",
    "_notebook_errors = []\n",
    "\n",
    "def log_error(error_msg, exc=None):\n",
    "    \"\"\"Log an error for later reporting. Does not raise.\"\"\"\n",
    "    import traceback\n",
    "    entry = {\"error\": str(error_msg)}\n",
    "    if exc:\n",
    "        entry[\"traceback\"] = traceback.format_exc()\n",
    "    _notebook_errors.append(entry)\n",
    "    print(f\"ERROR LOGGED: {error_msg}\")\n",
    "\n",
    "# Get widget values\n",
    "data_size = dbutils.widgets.get(\"data_size\")\n",
    "node_type = dbutils.widgets.get(\"node_type\")\n",
    "gpu_id = dbutils.widgets.get(\"gpu_id\")\n",
    "run_mode = dbutils.widgets.get(\"run_mode\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name_override = dbutils.widgets.get(\"table_name\").strip()\n",
    "\n",
    "# Add repo root to sys.path\n",
    "import sys, os\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_root = \"/\".join(notebook_path.split(\"/\")[:-2])\n",
    "sys.path.insert(0, f\"/Workspace{repo_root}\")\n",
    "\n",
    "# Use shared size presets from src/config.py (single source of truth)\n",
    "from src.config import get_preset, PRESETS\n",
    "\n",
    "# Determine input table\n",
    "if table_name_override:\n",
    "    # Use explicit table name override\n",
    "    input_table = f\"{catalog}.{schema}.{table_name_override}\"\n",
    "    # Use override name in run name\n",
    "    data_size_label = table_name_override.replace(\"imbalanced_\", \"\")\n",
    "    preset = None  # No preset when using override\n",
    "else:\n",
    "    # Use preset-based table name\n",
    "    preset = get_preset(data_size)\n",
    "    table_suffix = preset.table_suffix\n",
    "    input_table = f\"{catalog}.{schema}.imbalanced_{table_suffix}\"\n",
    "    data_size_label = data_size\n",
    "\n",
    "# Run naming\n",
    "run_name = f\"smoke_gpu_{node_type}\" if run_mode == \"smoke\" else f\"{data_size_label}_gpu_{node_type}\"\n",
    "\n",
    "print(f\"Data size: {data_size}\")\n",
    "print(f\"Node type: {node_type}\")\n",
    "print(f\"GPU ID: {gpu_id}\")\n",
    "print(f\"Run mode: {run_mode}\")\n",
    "print(f\"Input table: {input_table}\")\n",
    "print(f\"Run name: {run_name}\")\n",
    "if preset:\n",
    "    print(f\"Preset: {preset.name} ({preset.rows:,} rows, {preset.total_features} features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validate_env import validate_environment\n",
    "validate_environment(\n",
    "    track=\"gpu-scaling\",\n",
    "    expected_workers=0,  # Single node with GPU\n",
    "    require_gpu=True,\n",
    ")\n",
    "\n",
    "# Also check GPU availability and capture GPU info for later logging\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"GPU(s) detected:\\n{result.stdout}\")\n",
    "    gpu_name = result.stdout.strip().split(\",\")[0].strip()\n",
    "    gpu_mem_total = float(result.stdout.split(\",\")[1].strip().split()[0])  # MiB\n",
    "    print(f\"GPU name: {gpu_name}\")\n",
    "    print(f\"GPU memory: {gpu_mem_total/1024:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU detected! Use GPU ML Runtime (17.3.x-gpu-ml-scala2.13)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable system metrics logging BEFORE importing mlflow\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Also call the enable function after import\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# Get current user for experiment path\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{user_email}/xgb_scaling_benchmark\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment: {experiment_path}\")\n",
    "print(f\"System metrics logging enabled: {os.environ.get('MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Memory Check ---\n",
    "# Warn if estimated data size exceeds available RAM before loading.\n",
    "import psutil\n",
    "\n",
    "if preset:\n",
    "    estimated_gb = (preset.rows * preset.total_features * 8) / 1e9  # 8 bytes per float64\n",
    "    available_gb = psutil.virtual_memory().available / 1e9\n",
    "    print(f\"Estimated data size: {estimated_gb:.1f} GB\")\n",
    "    print(f\"Available RAM: {available_gb:.1f} GB\")\n",
    "    if estimated_gb > available_gb * 0.8:\n",
    "        msg = f\"WARNING: Estimated data size {estimated_gb:.1f} GB exceeds 80% of available RAM {available_gb:.1f} GB\"\n",
    "        print(msg)\n",
    "        log_error(msg)\n",
    "    else:\n",
    "        print(f\"Memory check OK: {estimated_gb:.1f} GB < {available_gb * 0.8:.1f} GB (80% threshold)\")\n",
    "else:\n",
    "    print(\"Memory check skipped: using table_name override (no preset dimensions available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Loading data from: {input_table}\")\n",
    "load_start = time.time()\n",
    "\n",
    "# Read from Delta table and convert to pandas\n",
    "df_spark = spark.table(input_table)\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"Loaded {len(df):,} rows x {len(df.columns)} columns in {load_time:.1f}s\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Estimate GPU memory needed for DMatrix\n",
    "# XGBoost GPU hist needs roughly 4-8x the raw data size for histograms\n",
    "raw_data_gb = df.memory_usage(deep=True).sum() / 1e9\n",
    "estimated_gpu_gb = raw_data_gb * 6  # Conservative estimate\n",
    "print(f\"Raw data: {raw_data_gb:.2f} GB\")\n",
    "print(f\"Estimated GPU memory needed: {estimated_gpu_gb:.2f} GB\")\n",
    "if estimated_gpu_gb > gpu_mem_total / 1024:\n",
    "    print(f\"WARNING: Estimated GPU memory {estimated_gpu_gb:.1f} GB may exceed GPU memory {gpu_mem_total/1024:.1f} GB\")\n",
    "    print(\"Consider using external_memory mode or a larger GPU\")\n",
    "else:\n",
    "    print(f\"GPU memory check OK: {estimated_gpu_gb:.1f} GB < {gpu_mem_total/1024:.1f} GB\")\n",
    "\n",
    "# Create MLflow dataset from pandas (avoids Spark credential scope issues)\n",
    "mlflow_dataset = mlflow.data.from_pandas(\n",
    "    df,\n",
    "    source=input_table,  # Reference source table\n",
    "    name=data_size_label,\n",
    "    targets=\"label\",\n",
    ")\n",
    "print(f\"MLflow dataset created: {mlflow_dataset.name} (source: {input_table})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and label\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Class distribution\n",
    "class_counts = y.value_counts().sort_index()\n",
    "minority_ratio = class_counts[1] / len(y)\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Class 0 (majority): {class_counts[0]:,} ({class_counts[0]/len(y)*100:.2f}%)\")\n",
    "print(f\"  Class 1 (minority): {class_counts[1]:,} ({class_counts[1]/len(y)*100:.2f}%)\")\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "print(f\"\\nscale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} rows\")\n",
    "print(f\"Test set: {len(X_test):,} rows\")\n",
    "print(f\"Train minority: {y_train.sum():,} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test minority: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# XGBoost hyperparameters - GPU accelerated with histogram method\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"gpu_id\": int(gpu_id),\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "print(\"XGBoost GPU parameters:\")\n",
    "for k, v in xgb_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run and train\n",
    "with mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow run ID: {run_id}\")\n",
    "    print(f\"MLflow run name: {run_name}\")\n",
    "\n",
    "    # Log input dataset\n",
    "    mlflow.log_input(mlflow_dataset, context=\"training\")\n",
    "    print(f\"Logged input dataset: {input_table}\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"data_size\", data_size)\n",
    "    mlflow.log_param(\"node_type\", node_type)\n",
    "    mlflow.log_param(\"run_mode\", run_mode)\n",
    "    mlflow.log_param(\"input_table\", input_table)\n",
    "    mlflow.log_param(\"n_rows\", len(df))\n",
    "    mlflow.log_param(\"n_features\", X.shape[1])\n",
    "    mlflow.log_param(\"minority_ratio\", round(minority_ratio, 4))\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "\n",
    "    # Log GPU-specific parameters\n",
    "    mlflow.log_param(\"training_mode\", \"gpu_single\")\n",
    "    mlflow.log_param(\"gpu_type\", gpu_name)\n",
    "    mlflow.log_param(\"gpu_memory_gb\", round(gpu_mem_total / 1024, 1))\n",
    "    mlflow.log_param(\"tree_method\", \"gpu_hist\")\n",
    "    mlflow.log_param(\"gpu_id\", int(gpu_id))\n",
    "\n",
    "    # Log XGBoost params\n",
    "    for k, v in xgb_params.items():\n",
    "        mlflow.log_param(f\"xgb_{k}\", v)\n",
    "\n",
    "    # Log data load time\n",
    "    mlflow.log_metric(\"data_load_time_sec\", load_time)\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nTraining XGBoost with GPU (tree_method=gpu_hist)...\")\n",
    "    train_start = time.time()\n",
    "\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_time = time.time() - train_start\n",
    "    print(f\"Training completed in {train_time:.1f}s\")\n",
    "\n",
    "    mlflow.log_metric(\"train_time_sec\", train_time)\n",
    "\n",
    "    # Predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    pred_start = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    pred_time = time.time() - pred_start\n",
    "    mlflow.log_metric(\"predict_time_sec\", pred_time)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nEvaluating...\")\n",
    "    from sklearn.metrics import (\n",
    "        average_precision_score,\n",
    "        roc_auc_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        classification_report,\n",
    "        confusion_matrix,\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    auc_pr = average_precision_score(y_test, y_pred_proba)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  AUC-PR (primary): {auc_pr:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}\")\n",
    "    print(f\"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}\")\n",
    "\n",
    "    mlflow.log_metric(\"true_negatives\", cm[0, 0])\n",
    "    mlflow.log_metric(\"false_positives\", cm[0, 1])\n",
    "    mlflow.log_metric(\"false_negatives\", cm[1, 0])\n",
    "    mlflow.log_metric(\"true_positives\", cm[1, 1])\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Log GPU memory estimate\n",
    "    mlflow.log_metric(\"raw_data_gb\", round(raw_data_gb, 2))\n",
    "    mlflow.log_metric(\"estimated_gpu_mem_gb\", round(estimated_gpu_gb, 2))\n",
    "\n",
    "    # Total time\n",
    "    total_time = load_time + train_time + pred_time\n",
    "    mlflow.log_metric(\"total_time_sec\", total_time)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Run complete: {run_name}\")\n",
    "    print(f\"Total time: {total_time:.1f}s\")\n",
    "    print(f\"MLflow run ID: {run_id}\")\n",
    "    print(f\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = {\n",
    "    \"status\": \"ok\" if not _notebook_errors else \"errors\",\n",
    "    \"run_name\": run_name,\n",
    "    \"run_id\": run_id,\n",
    "    \"data_size\": data_size,\n",
    "    \"node_type\": node_type,\n",
    "    \"gpu_type\": gpu_name,\n",
    "    \"gpu_id\": int(gpu_id),\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"n_rows\": len(df),\n",
    "    \"auc_pr\": round(auc_pr, 4),\n",
    "    \"train_time_sec\": round(train_time, 1),\n",
    "    \"total_time_sec\": round(total_time, 1),\n",
    "}\n",
    "\n",
    "if _notebook_errors:\n",
    "    result[\"errors\"] = _notebook_errors\n",
    "\n",
    "result_json = json.dumps(result)\n",
    "print(f\"\\nNotebook result: {result_json}\")\n",
    "\n",
    "dbutils.notebook.exit(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}