{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost (Ray on Spark - Plasma Object Store Tuning)\n",
    "\n",
    "Distributed XGBoost training using Ray on Spark with **configurable Plasma/Object Store parameters**\n",
    "and **per-worker system metrics collection**.\n",
    "\n",
    "This notebook extends `train_xgb_ray.ipynb` with additional widget parameters for tuning Ray's\n",
    "object store (shared memory), spilling configuration, and heap memory allocation.\n",
    "\n",
    "**Experiment Goal:** Find optimal object store configuration for 10M+ row datasets.\n",
    "\n",
    "**New Parameters (vs base notebook):**\n",
    "- `obj_store_mem_gb`: Object store memory per worker node in GB (0 = Ray default ~30% of RAM)\n",
    "- `head_obj_store_mem_gb`: Object store memory for head node in GB (0 = Ray default)\n",
    "- `heap_mem_gb`: Heap memory per worker in GB (0 = Ray default)\n",
    "- `spill_dir`: Object spilling directory path (default: /local_disk0/ray_spill)\n",
    "- `ray_temp_dir`: Ray temp root directory (default: /local_disk0/tmp)\n",
    "- `allow_slow_storage`: Set RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE=1 to bypass /dev/shm cap\n",
    "\n",
    "**Worker System Metrics:**\n",
    "- Deploys `SystemMetricsMonitor` Ray actors on each worker node\n",
    "- Logs CPU, memory, disk, network metrics per worker to the same MLflow run\n",
    "- Metric names: `system/worker_0/cpu_utilization_percentage`, `system/worker_1/...`, etc.\n",
    "- Credentials passed from driver via `dbutils` context to workers via Ray object store\n",
    "\n",
    "**Requirements:**\n",
    "- Databricks ML Runtime 17.3 LTS (includes Ray 2.37.0)\n",
    "- Multi-node cluster (2+ workers recommended)\n",
    "\n",
    "**MLflow:**\n",
    "- System metrics enabled (driver + all worker nodes)\n",
    "- All Plasma config params logged for experiment comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global error tracking - captures errors from any cell\n",
    "_notebook_errors = []\n",
    "\n",
    "def log_error(error_msg, exc=None):\n",
    "    \"\"\"Log an error for later retrieval in exit cell.\"\"\"\n",
    "    import traceback\n",
    "    entry = {\"error\": str(error_msg)}\n",
    "    if exc:\n",
    "        entry[\"traceback\"] = traceback.format_exc()\n",
    "    _notebook_errors.append(entry)\n",
    "    print(f\"ERROR LOGGED: {error_msg}\")\n",
    "\n",
    "# === Standard parameters (same as train_xgb_ray.ipynb) ===\n",
    "dbutils.widgets.dropdown(\"data_size\", \"tiny\", [\"tiny\", \"small\", \"medium\", \"large\", \"xlarge\"], \"Data Size\")\n",
    "dbutils.widgets.text(\"node_type\", \"D8sv5\", \"Node Type\")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"full\", [\"full\", \"smoke\"], \"Run Mode\")\n",
    "dbutils.widgets.text(\"num_workers\", \"0\", \"Num Workers (0=auto)\")\n",
    "dbutils.widgets.text(\"cpus_per_worker\", \"0\", \"CPUs per Worker (0=auto)\")\n",
    "dbutils.widgets.text(\"warehouse_id\", \"148ccb90800933a1\", \"Databricks SQL Warehouse ID\")\n",
    "dbutils.widgets.text(\"catalog\", \"brian_gen_ai\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"xgb_scaling\", \"Schema\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Table Name (override)\")\n",
    "\n",
    "# === NEW: Plasma / Object Store tuning parameters ===\n",
    "dbutils.widgets.text(\"obj_store_mem_gb\", \"0\", \"Object Store Memory per Worker (GB, 0=default)\")\n",
    "dbutils.widgets.text(\"head_obj_store_mem_gb\", \"0\", \"Object Store Memory Head (GB, 0=default)\")\n",
    "dbutils.widgets.text(\"heap_mem_gb\", \"0\", \"Heap Memory per Worker (GB, 0=default)\")\n",
    "dbutils.widgets.text(\"spill_dir\", \"/local_disk0/ray_spill\", \"Object Spill Directory\")\n",
    "dbutils.widgets.text(\"ray_temp_dir\", \"/local_disk0/tmp\", \"Ray Temp Root Directory\")\n",
    "dbutils.widgets.dropdown(\"allow_slow_storage\", \"0\", [\"0\", \"1\"], \"Allow Slow Storage (bypass /dev/shm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "data_size = dbutils.widgets.get(\"data_size\")\n",
    "node_type = dbutils.widgets.get(\"node_type\")\n",
    "run_mode = dbutils.widgets.get(\"run_mode\")\n",
    "num_workers_input = int(dbutils.widgets.get(\"num_workers\"))\n",
    "cpus_per_worker_input = int(dbutils.widgets.get(\"cpus_per_worker\"))\n",
    "warehouse_id = dbutils.widgets.get(\"warehouse_id\").strip()\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name_override = dbutils.widgets.get(\"table_name\").strip()\n",
    "\n",
    "# NEW: Plasma config values\n",
    "obj_store_mem_gb = float(dbutils.widgets.get(\"obj_store_mem_gb\"))\n",
    "head_obj_store_mem_gb = float(dbutils.widgets.get(\"head_obj_store_mem_gb\"))\n",
    "heap_mem_gb = float(dbutils.widgets.get(\"heap_mem_gb\"))\n",
    "spill_dir = dbutils.widgets.get(\"spill_dir\").strip()\n",
    "ray_temp_dir = dbutils.widgets.get(\"ray_temp_dir\").strip()\n",
    "allow_slow_storage = dbutils.widgets.get(\"allow_slow_storage\").strip()\n",
    "\n",
    "# Convert GB to bytes (0 means use Ray default)\n",
    "obj_store_mem_bytes = int(obj_store_mem_gb * 1024 * 1024 * 1024) if obj_store_mem_gb > 0 else None\n",
    "head_obj_store_mem_bytes = int(head_obj_store_mem_gb * 1024 * 1024 * 1024) if head_obj_store_mem_gb > 0 else None\n",
    "heap_mem_bytes = int(heap_mem_gb * 1024 * 1024 * 1024) if heap_mem_gb > 0 else None\n",
    "\n",
    "# Dataset size preset mapping\n",
    "SIZE_PRESETS = {\n",
    "    \"tiny\": {\"suffix\": \"10k\", \"rows\": 10_000, \"features\": 20},\n",
    "    \"small\": {\"suffix\": \"1m\", \"rows\": 1_000_000, \"features\": 100},\n",
    "    \"medium\": {\"suffix\": \"10m\", \"rows\": 10_000_000, \"features\": 250},\n",
    "    \"large\": {\"suffix\": \"100m\", \"rows\": 100_000_000, \"features\": 500},\n",
    "    \"xlarge\": {\"suffix\": \"500m\", \"rows\": 500_000_000, \"features\": 500},\n",
    "}\n",
    "\n",
    "# Determine input table\n",
    "if table_name_override:\n",
    "    input_table = f\"{catalog}.{schema}.{table_name_override}\"\n",
    "    data_size_label = table_name_override.replace(\"imbalanced_\", \"\")\n",
    "else:\n",
    "    preset = SIZE_PRESETS[data_size]\n",
    "    table_suffix = preset[\"suffix\"]\n",
    "    input_table = f\"{catalog}.{schema}.imbalanced_{table_suffix}\"\n",
    "    data_size_label = data_size\n",
    "\n",
    "# Build a compact plasma config tag for the run name\n",
    "plasma_tag = f\"os{obj_store_mem_gb:.0f}g\" if obj_store_mem_gb > 0 else \"osD\"\n",
    "if heap_mem_gb > 0:\n",
    "    plasma_tag += f\"_h{heap_mem_gb:.0f}g\"\n",
    "if allow_slow_storage == \"1\":\n",
    "    plasma_tag += \"_slow\"\n",
    "\n",
    "# Run naming (prefix with ray_ and worker config + plasma config)\n",
    "if run_mode == \"smoke\":\n",
    "    run_name = f\"plasma_smoke_{node_type}\"\n",
    "else:\n",
    "    worker_suffix = f\"_{num_workers_input}w\" if num_workers_input > 0 else \"\"\n",
    "    run_name = f\"plasma_{data_size_label}{worker_suffix}_{node_type}_{plasma_tag}\"\n",
    "\n",
    "print(f\"Data size: {data_size}\")\n",
    "print(f\"Node type: {node_type}\")\n",
    "print(f\"Run mode: {run_mode}\")\n",
    "print(f\"Num workers input: {num_workers_input} (0=auto)\")\n",
    "print(f\"CPUs per worker input: {cpus_per_worker_input} (0=auto)\")\n",
    "print(f\"Warehouse ID: {warehouse_id}\")\n",
    "print(f\"Input table: {input_table}\")\n",
    "print(f\"Run name: {run_name}\")\n",
    "print(f\"\\n--- Plasma Object Store Config ---\")\n",
    "print(f\"  obj_store_mem_gb: {obj_store_mem_gb} ({obj_store_mem_bytes} bytes)\")\n",
    "print(f\"  head_obj_store_mem_gb: {head_obj_store_mem_gb} ({head_obj_store_mem_bytes} bytes)\")\n",
    "print(f\"  heap_mem_gb: {heap_mem_gb} ({heap_mem_bytes} bytes)\")\n",
    "print(f\"  spill_dir: {spill_dir}\")\n",
    "print(f\"  ray_temp_dir: {ray_temp_dir}\")\n",
    "print(f\"  allow_slow_storage: {allow_slow_storage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable system metrics logging BEFORE importing mlflow\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "\n",
    "# Set slow storage env var if requested (must be set BEFORE Ray starts)\n",
    "if allow_slow_storage == \"1\":\n",
    "    os.environ[\"RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE\"] = \"1\"\n",
    "    print(\"RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE=1 (bypassing /dev/shm cap)\")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Also call the enable function after import\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# Get current user for experiment path\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{user_email}/xgb_scaling_benchmark\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment: {experiment_path}\")\n",
    "print(f\"System metrics logging enabled: {os.environ.get('MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray on Spark (with Plasma Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport json as _json\n\n# Check Ray version and availability\ntry:\n    import ray\n    print(f\"Ray version: {ray.__version__}\")\nexcept ImportError as e:\n    raise RuntimeError(f\"Ray not available: {e}\")\n\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n\n# Setup Databricks environment for Ray workers (required for MLflow + Ray Data access)\ndatabricks_host_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\ndatabricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n\n# Ray Data Databricks reader expects hostname without scheme.\ndatabricks_host = databricks_host_url.replace(\"https://\", \"\").replace(\"http://\", \"\").rstrip(\"/\")\n\n# Default env setup for MLflow and general Databricks SDK usage.\nos.environ[\"DATABRICKS_HOST\"] = databricks_host_url\nos.environ[\"DATABRICKS_TOKEN\"] = databricks_token\n\nprint(f\"Databricks Host URL: {databricks_host_url}\")\nprint(f\"Databricks Hostname (Ray Data): {databricks_host}\")\nprint(\"Databricks Token: [CONFIGURED]\")\n\n# Get cluster info\nsc = spark.sparkContext\nnum_executors = sc._jsc.sc().getExecutorMemoryStatus().size() - 1  # Exclude driver\nprint(f\"\\nSpark executors (workers): {num_executors}\")\n\nif num_executors < 1:\n    print(\"WARNING: No executors detected. This may indicate a single-node cluster.\")\n    print(\"Ray on Spark requires a multi-node cluster (num_workers >= 1)\")\n\n# Determine per-node vCPU and leave one CPU for Spark/system buffer\nimport re\nnode_type_lower = node_type.lower()\nnode_vcpus_match = re.search(r\"[de](\\d+)\", node_type_lower)\nnode_vcpus = int(node_vcpus_match.group(1)) if node_vcpus_match else 8\nallocatable_cpus_per_node = max(1, node_vcpus - 1)\n\n# Override OMP_NUM_THREADS — Databricks/Spark sets this to 1 on executors,\n# which silently caps XGBoost nthread to 1 regardless of what you set.\n# XGBoost's C++ layer does: min(nthread, omp_get_max_threads()) = min(14, 1) = 1\n# Setting it here on the driver propagates to Ray workers spawned by setup_ray_cluster.\nos.environ[\"OMP_NUM_THREADS\"] = str(allocatable_cpus_per_node)\nprint(f\"\\nOMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']} (overriding Spark default of 1)\")\n\n# Determine number of Ray workers (best practice: one worker per executor)\nif num_workers_input > 0:\n    num_workers = num_workers_input\nelse:\n    num_workers = max(1, num_executors)\n\n# Keep worker count aligned with available executors\nif num_workers > num_executors:\n    print(f\"Capping num_workers from {num_workers} to {num_executors} (executor count)\")\n    num_workers = num_executors\n\n# CPUs per worker for Ray training\nif cpus_per_worker_input > 0:\n    cpus_per_worker = cpus_per_worker_input\nelse:\n    cpus_per_worker = allocatable_cpus_per_node\n\n# Enforce per-node CPU safety cap\nif cpus_per_worker > allocatable_cpus_per_node:\n    print(\n        f\"Capping cpus_per_worker from {cpus_per_worker} to {allocatable_cpus_per_node} \"\n        f\"for node type {node_type}\"\n    )\n    cpus_per_worker = allocatable_cpus_per_node\n\n# Use the same per-node CPU for Ray cluster worker allocation\nnum_cpus_worker_node = allocatable_cpus_per_node\n\nprint(f\"\\nResource sizing:\")\nprint(f\"  node_type: {node_type}\")\nprint(f\"  node_vcpus: {node_vcpus}\")\nprint(f\"  allocatable_cpus_per_node: {allocatable_cpus_per_node}\")\nprint(f\"  spark_executors: {num_executors}\")\nprint(f\"\\nRay training configuration:\")\nprint(f\"  num_workers: {num_workers}\")\nprint(f\"  cpus_per_worker: {cpus_per_worker}\")\nprint(f\"  total_requested_cpus: {num_workers * cpus_per_worker}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Ray cluster on Spark WITH Plasma object store tuning\n# AND runtime_env to set OMP_NUM_THREADS at process level (before any imports)\nprint(\"Starting Ray cluster on Spark (with Plasma tuning + OMP fix)...\")\nray_start = time.time()\n\n# --- Build head_node_options with spilling config ---\nhead_node_options = {}\nif spill_dir:\n    head_node_options[\"system_config\"] = {\n        \"object_spilling_config\": _json.dumps({\n            \"type\": \"filesystem\",\n            \"params\": {\n                \"directory_path\": spill_dir\n            }\n        })\n    }\n    print(f\"Custom spill directory: {spill_dir}\")\n\n# --- Collect /dev/shm size for diagnostics ---\ntry:\n    import subprocess\n    shm_info = subprocess.run([\"df\", \"-h\", \"/dev/shm\"], capture_output=True, text=True)\n    print(f\"\\n/dev/shm on driver:\\n{shm_info.stdout}\")\nexcept Exception:\n    pass\n\n# --- Build setup_ray_cluster kwargs ---\nray_cluster_kwargs = {\n    \"min_worker_nodes\": num_executors,           # Fixed size cluster for benchmarks\n    \"max_worker_nodes\": num_executors,           # No autoscaling for benchmarks\n    \"num_cpus_worker_node\": num_cpus_worker_node,\n    \"num_gpus_worker_node\": 0,                   # CPU-only training\n    \"collect_log_to_path\": \"/tmp/ray_logs\",\n}\n\n# Plasma tuning parameters (only set if non-default)\nif obj_store_mem_bytes is not None:\n    ray_cluster_kwargs[\"object_store_memory_worker_node\"] = obj_store_mem_bytes\n    print(f\"Object store memory per worker: {obj_store_mem_gb:.1f} GB ({obj_store_mem_bytes:,} bytes)\")\n\nif head_obj_store_mem_bytes is not None:\n    ray_cluster_kwargs[\"object_store_memory_head_node\"] = head_obj_store_mem_bytes\n    print(f\"Object store memory head node: {head_obj_store_mem_gb:.1f} GB ({head_obj_store_mem_bytes:,} bytes)\")\n\nif heap_mem_bytes is not None:\n    ray_cluster_kwargs[\"memory_worker_node\"] = heap_mem_bytes\n    print(f\"Heap memory per worker: {heap_mem_gb:.1f} GB ({heap_mem_bytes:,} bytes)\")\n\nif ray_temp_dir:\n    ray_cluster_kwargs[\"ray_temp_root_dir\"] = ray_temp_dir\n    print(f\"Ray temp root dir: {ray_temp_dir}\")\n\nif head_node_options:\n    ray_cluster_kwargs[\"head_node_options\"] = head_node_options\n\nprint(f\"\\nsetup_ray_cluster kwargs:\")\nfor k, v in ray_cluster_kwargs.items():\n    if k == \"head_node_options\":\n        print(f\"  {k}: {_json.dumps(v, indent=4)}\")\n    else:\n        print(f\"  {k}: {v}\")\n\ntry:\n    ray_cluster = setup_ray_cluster(**ray_cluster_kwargs)\n    ray_init_time = time.time() - ray_start\n    print(f\"\\nRay cluster initialized in {ray_init_time:.1f}s\")\n\n    # =======================================================================\n    # CRITICAL OMP FIX: Reconnect to Ray with runtime_env that sets\n    # OMP_NUM_THREADS at the OS level BEFORE the worker Python process starts.\n    #\n    # setup_ray_cluster() internally calls ray.init() for health checks then\n    # ray.shutdown(), leaving RAY_ADDRESS set but no active connection.\n    # We reconnect with runtime_env so ALL Ray workers (tasks + actors)\n    # inherit OMP_NUM_THREADS=<cpus_per_worker> from process startup.\n    #\n    # This is the MOST RELIABLE fix because:\n    # - The env var is set via os.execvp() before the Python interpreter starts\n    # - OpenMP runtime reads it during first library load (import xgboost)\n    # - No race condition, no wrong-library problem, no caching issue\n    # =======================================================================\n    omp_threads_str = str(cpus_per_worker)\n    print(f\"\\nReconnecting to Ray with runtime_env OMP_NUM_THREADS={omp_threads_str}...\")\n    \n    # Disconnect if setup_ray_cluster left a connection open\n    if ray.is_initialized():\n        ray.shutdown()\n    \n    ray.init(\n        runtime_env={\n            \"env_vars\": {\n                \"OMP_NUM_THREADS\": omp_threads_str,\n                # Also propagate Databricks auth for MLflow/Ray Data\n                \"DATABRICKS_HOST\": databricks_host_url,\n                \"DATABRICKS_TOKEN\": databricks_token,\n            }\n        }\n    )\n    print(f\"Ray reconnected with runtime_env. OMP_NUM_THREADS={omp_threads_str}\")\n    print(f\"Ray head node: {ray.get_runtime_context().get_node_id()[:8]}\")\n    # =======================================================================\n\n    print(f\"\\nRay cluster resources:\")\n    cluster_resources = ray.cluster_resources()\n    print(cluster_resources)\n\n    # Collect actual object store size from Ray nodes for diagnostics\n    try:\n        nodes_info = ray.nodes()\n        for node in nodes_info:\n            node_id_short = node.get('NodeID', 'unknown')[:8]\n            resources = node.get('Resources', {})\n            obj_store = resources.get('object_store_memory', 0)\n            mem = resources.get('memory', 0)\n            cpus = resources.get('CPU', 0)\n            alive = node.get('Alive', False)\n            print(f\"  Node {node_id_short}: CPU={cpus}, memory={mem/(1024**3):.1f}GB, object_store={obj_store/(1024**3):.2f}GB, alive={alive}\")\n    except Exception as e:\n        print(f\"  (Could not collect node details: {e})\")\n\n    # Preflight CPU validation to avoid trainer stalls from pending actors\n    AUTO_CAP_RESOURCES = True\n    ray_train_overhead_cpus = 1\n    available_cpus = int(cluster_resources.get(\"CPU\", 0))\n    required_worker_cpus = int(num_workers * cpus_per_worker)\n    required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n\n    print(\"\\nRay preflight CPU check:\")\n    print(f\"  available_cpus: {available_cpus}\")\n    print(f\"  requested_worker_cpus: {required_worker_cpus} ({num_workers} workers x {cpus_per_worker} CPU)\")\n    print(f\"  ray_train_overhead_cpus: {ray_train_overhead_cpus}\")\n    print(f\"  requested_total_cpus: {required_total_cpus}\")\n\n    if required_total_cpus > available_cpus:\n        if not AUTO_CAP_RESOURCES:\n            raise RuntimeError(\n                f\"Insufficient Ray CPUs: requested total {required_total_cpus}, available {available_cpus}. \"\n                \"Reduce num_workers/cpus_per_worker or increase cluster size.\"\n            )\n\n        usable_cpus_for_workers = max(1, available_cpus - ray_train_overhead_cpus)\n        capped_cpus_per_worker = max(1, usable_cpus_for_workers // max(1, num_workers))\n        if capped_cpus_per_worker < cpus_per_worker:\n            print(f\"  auto-cap: cpus_per_worker {cpus_per_worker} -> {capped_cpus_per_worker}\")\n            cpus_per_worker = capped_cpus_per_worker\n\n        required_worker_cpus = int(num_workers * cpus_per_worker)\n        required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n        if required_total_cpus > available_cpus:\n            capped_workers = max(1, usable_cpus_for_workers // max(1, cpus_per_worker))\n            if capped_workers < num_workers:\n                print(f\"  auto-cap: num_workers {num_workers} -> {capped_workers}\")\n                num_workers = capped_workers\n\n        required_worker_cpus = int(num_workers * cpus_per_worker)\n        required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n        if required_total_cpus > available_cpus:\n            raise RuntimeError(\n                f\"Unable to fit Ray resources after auto-cap: requested total {required_total_cpus}, available {available_cpus}.\"\n            )\n\n    print(f\"  final_num_workers: {num_workers}\")\n    print(f\"  final_cpus_per_worker: {cpus_per_worker}\")\n    print(f\"  final_requested_worker_cpus: {num_workers * cpus_per_worker}\")\n    print(f\"  final_requested_total_cpus: {(num_workers * cpus_per_worker) + ray_train_overhead_cpus}\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to initialize Ray cluster: {e}\")\n    print(f\"\\nDebug info:\")\n    print(f\"  Spark version: {spark.version}\")\n    print(f\"  Executors: {num_executors}\")\n    import traceback\n    traceback.print_exc()\n    raise"
  },
  {
   "cell_type": "markdown",
   "source": "## Worker-Side System Metrics Collection\n\nDeploy MLflow `SystemMetricsMonitor` actors on each Ray worker node to collect per-node CPU, memory,\ndisk, and network metrics. Each worker logs to the **same MLflow run** as the driver but with a unique\n`node_id` prefix (e.g., `system/worker_0/cpu_utilization_percentage`).\n\n**Key design decisions:**\n- Uses `SystemMetricsMonitor` directly (NOT `mlflow.start_run`) to avoid side-effects on run status\n- Passes Databricks auth credentials from driver via `ray.put()` to the object store\n- Workers set `DATABRICKS_HOST`, `DATABRICKS_TOKEN`, and `MLFLOW_TRACKING_URI` env vars for MLflow auth\n- Each actor is pinned to a unique worker node via Ray scheduling\n- Monitors are started before training and stopped after, with `try/finally` for clean shutdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Ray actor for per-worker MLflow system metrics collection.\n",
    "# This actor runs SystemMetricsMonitor directly — it does NOT call mlflow.start_run(),\n",
    "# so it won't interfere with the driver's run status. It only logs system metrics\n",
    "# (CPU, memory, disk, network) to the same run_id with a unique node_id prefix.\n",
    "#\n",
    "# IMPORTANT: We do NOT pass tracking_uri to SystemMetricsMonitor constructor because\n",
    "# that parameter only exists in unreleased MLflow (master). Instead, we set the\n",
    "# MLFLOW_TRACKING_URI environment variable before creating the monitor, which MLflow\n",
    "# reads internally via mlflow.get_tracking_uri().\n",
    "\n",
    "import ray\n",
    "\n",
    "@ray.remote(num_cpus=0)  # Zero CPU requirement — monitoring only, no resource reservation\n",
    "class WorkerMetricsMonitor:\n",
    "    \"\"\"Runs MLflow SystemMetricsMonitor on a Ray worker node.\n",
    "    \n",
    "    Logs system metrics to an existing MLflow run with a unique node_id prefix.\n",
    "    Uses SystemMetricsMonitor directly (not mlflow.start_run) to avoid side effects.\n",
    "    \n",
    "    Metric names produced: system/{node_id}/cpu_utilization_percentage, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, run_id: str, node_id: str, db_host: str, db_token: str,\n",
    "                 sampling_interval: float = 10.0):\n",
    "        import os\n",
    "        \n",
    "        # Set Databricks auth env vars so MLflow can talk to the tracking server.\n",
    "        # These must be set BEFORE creating SystemMetricsMonitor because the monitor\n",
    "        # internally creates a BatchMetricsLogger which reads mlflow.get_tracking_uri().\n",
    "        os.environ[\"DATABRICKS_HOST\"] = db_host\n",
    "        os.environ[\"DATABRICKS_TOKEN\"] = db_token\n",
    "        os.environ[\"MLFLOW_TRACKING_URI\"] = \"databricks\"\n",
    "        \n",
    "        self._run_id = run_id\n",
    "        self._node_id = node_id\n",
    "        self._sampling_interval = sampling_interval\n",
    "        self._monitor = None\n",
    "        \n",
    "        # Capture the Ray node ID for diagnostics\n",
    "        self._ray_node_id = ray.get_runtime_context().get_node_id()[:8]\n",
    "        \n",
    "    def start(self) -> str:\n",
    "        \"\"\"Start collecting system metrics. Returns node info string.\"\"\"\n",
    "        from mlflow.system_metrics.system_metrics_monitor import SystemMetricsMonitor\n",
    "        \n",
    "        self._monitor = SystemMetricsMonitor(\n",
    "            run_id=self._run_id,\n",
    "            node_id=self._node_id,\n",
    "            sampling_interval=self._sampling_interval,\n",
    "            samples_before_logging=1,  # Log every sample for fine-grained visibility\n",
    "        )\n",
    "        self._monitor.start()\n",
    "        return f\"{self._node_id} started on ray_node={self._ray_node_id}\"\n",
    "    \n",
    "    def stop(self) -> str:\n",
    "        \"\"\"Stop collecting and flush remaining metrics.\"\"\"\n",
    "        if self._monitor is not None:\n",
    "            self._monitor.finish()\n",
    "            self._monitor = None\n",
    "            return f\"{self._node_id} stopped on ray_node={self._ray_node_id}\"\n",
    "        return f\"{self._node_id} was not running\"\n",
    "    \n",
    "    def status(self) -> dict:\n",
    "        \"\"\"Return current monitoring status.\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self._node_id,\n",
    "            \"ray_node\": self._ray_node_id,\n",
    "            \"running\": self._monitor is not None,\n",
    "        }\n",
    "\n",
    "\n",
    "def start_worker_monitors(run_id: str, db_host: str, db_token: str, \n",
    "                           num_nodes: int, sampling_interval: float = 10.0):\n",
    "    \"\"\"Launch WorkerMetricsMonitor actors across Ray worker nodes.\n",
    "    \n",
    "    Creates one monitor actor per worker node. Each actor is scheduled\n",
    "    on a specific node via NodeAffinitySchedulingStrategy.\n",
    "    \n",
    "    Args:\n",
    "        run_id: MLflow run ID to log metrics to\n",
    "        db_host: Databricks workspace URL (with https://)\n",
    "        db_token: Databricks API token\n",
    "        num_nodes: Number of Ray worker nodes to monitor\n",
    "        sampling_interval: Seconds between metric samples\n",
    "        \n",
    "    Returns:\n",
    "        List of actor handles (keep references alive!)\n",
    "    \"\"\"\n",
    "    # Get unique alive worker node IDs (exclude head node)\n",
    "    head_node_id = ray.get_runtime_context().get_node_id()\n",
    "    alive_nodes = [n for n in ray.nodes() if n.get(\"Alive\") and n[\"NodeID\"] != head_node_id]\n",
    "    \n",
    "    # Cap to requested number of nodes\n",
    "    target_nodes = alive_nodes[:num_nodes]\n",
    "    \n",
    "    print(f\"Starting worker metrics monitors for {len(target_nodes)} nodes...\")\n",
    "    print(f\"  Head node (excluded): {head_node_id[:8]}\")\n",
    "    \n",
    "    actors = []\n",
    "    start_futures = []\n",
    "    \n",
    "    for idx, node_info in enumerate(target_nodes):\n",
    "        node_id_label = f\"worker_{idx}\"\n",
    "        ray_node_id = node_info[\"NodeID\"]\n",
    "        \n",
    "        # Schedule actor on this specific node using node affinity\n",
    "        actor = WorkerMetricsMonitor.options(\n",
    "            scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(\n",
    "                node_id=ray_node_id,\n",
    "                soft=False,\n",
    "            ),\n",
    "            name=f\"mlflow_metrics_{node_id_label}\",\n",
    "        ).remote(\n",
    "            run_id=run_id,\n",
    "            node_id=node_id_label,\n",
    "            db_host=db_host,\n",
    "            db_token=db_token,\n",
    "            sampling_interval=sampling_interval,\n",
    "        )\n",
    "        \n",
    "        start_futures.append(actor.start.remote())\n",
    "        actors.append(actor)\n",
    "    \n",
    "    # Wait for all monitors to start\n",
    "    results = ray.get(start_futures)\n",
    "    for r in results:\n",
    "        print(f\"  {r}\")\n",
    "    \n",
    "    print(f\"All {len(actors)} worker monitors started (sampling every {sampling_interval}s)\")\n",
    "    return actors\n",
    "\n",
    "\n",
    "def stop_worker_monitors(actors):\n",
    "    \"\"\"Stop all WorkerMetricsMonitor actors and flush metrics.\n",
    "    \n",
    "    Args:\n",
    "        actors: List of WorkerMetricsMonitor actor handles\n",
    "    \"\"\"\n",
    "    if not actors:\n",
    "        print(\"No worker monitors to stop.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Stopping {len(actors)} worker metrics monitors...\")\n",
    "    stop_futures = [actor.stop.remote() for actor in actors]\n",
    "    \n",
    "    try:\n",
    "        results = ray.get(stop_futures, timeout=30)\n",
    "        for r in results:\n",
    "            print(f\"  {r}\")\n",
    "    except ray.exceptions.GetTimeoutError:\n",
    "        print(\"  WARNING: Some monitors did not stop within 30s timeout\")\n",
    "    except Exception as e:\n",
    "        print(f\"  WARNING: Error stopping monitors: {e}\")\n",
    "    \n",
    "    # Kill the actors to free resources\n",
    "    for actor in actors:\n",
    "        try:\n",
    "            ray.kill(actor)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    print(\"Worker monitors stopped and cleaned up.\")\n",
    "\n",
    "\n",
    "print(\"WorkerMetricsMonitor actor defined.\")\n",
    "print(\"Helper functions: start_worker_monitors(), stop_worker_monitors()\")\n",
    "print(f\"\\nCredentials available for workers:\")\n",
    "print(f\"  DATABRICKS_HOST: {databricks_host_url[:40]}...\")\n",
    "print(f\"  DATABRICKS_TOKEN: [CONFIGURED]\")"
   ],
   "id": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.data\n",
    "\n",
    "if not warehouse_id:\n",
    "    raise ValueError(\"warehouse_id is required for distributed Ray Data loading\")\n",
    "\n",
    "print(f\"Loading data from: {input_table}\")\n",
    "print(f\"Using SQL Warehouse: {warehouse_id}\")\n",
    "load_start = time.time()\n",
    "\n",
    "query = f\"SELECT * FROM {input_table}\"\n",
    "\n",
    "# Ray Data's Databricks reader expects DATABRICKS_HOST without scheme.\n",
    "_original_db_host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = databricks_host\n",
    "try:\n",
    "    full_ray_ds = ray.data.read_databricks_tables(\n",
    "        warehouse_id=warehouse_id,\n",
    "        query=query,\n",
    "    )\n",
    "finally:\n",
    "    # Restore default host URL for downstream APIs.\n",
    "    if _original_db_host is not None:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = _original_db_host\n",
    "    else:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = databricks_host_url\n",
    "\n",
    "n_rows = full_ray_ds.count()\n",
    "all_columns = list(full_ray_ds.schema().names)\n",
    "if \"label\" not in all_columns:\n",
    "    raise ValueError(f\"Expected 'label' column in dataset schema, got: {all_columns}\")\n",
    "\n",
    "feature_columns = [c for c in all_columns if c != \"label\"]\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "print(f\"Loaded {n_rows:,} rows x {len(all_columns)} columns in {load_time:.1f}s\")\n",
    "print(f\"Feature count: {len(feature_columns)}\")\n",
    "\n",
    "# Create a lightweight MLflow input dataset sample (avoid driver OOM)\n",
    "mlflow_sample_rows = min(10_000, n_rows)\n",
    "mlflow_sample_df = full_ray_ds.limit(mlflow_sample_rows).to_pandas()\n",
    "mlflow_dataset = mlflow.data.from_pandas(\n",
    "    mlflow_sample_df,\n",
    "    source=input_table,\n",
    "    name=data_size_label,\n",
    "    targets=\"label\",\n",
    ")\n",
    "print(f\"MLflow dataset sample created: {mlflow_dataset.name} ({mlflow_sample_rows:,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution from Ray Dataset\n",
    "positive_count = int(full_ray_ds.sum(\"label\"))\n",
    "negative_count = int(n_rows - positive_count)\n",
    "minority_ratio = positive_count / n_rows if n_rows else 0.0\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Class 0 (majority): {negative_count:,} ({(negative_count / n_rows) * 100:.2f}%)\")\n",
    "print(f\"  Class 1 (minority): {positive_count:,} ({(positive_count / n_rows) * 100:.2f}%)\")\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = negative_count / max(positive_count, 1)\n",
    "print(f\"\\nscale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split in Ray Data (keeps ingestion distributed)\n",
    "split_start = time.time()\n",
    "train_ray_ds, test_ray_ds = full_ray_ds.train_test_split(test_size=0.2, seed=42)\n",
    "split_time = time.time() - split_start\n",
    "\n",
    "train_count = train_ray_ds.count()\n",
    "test_count = test_ray_ds.count()\n",
    "train_pos = int(train_ray_ds.sum(\"label\"))\n",
    "test_pos = int(test_ray_ds.sum(\"label\"))\n",
    "\n",
    "print(f\"Train set: {train_count:,} rows\")\n",
    "print(f\"Test set: {test_count:,} rows\")\n",
    "print(f\"Train minority: {train_pos:,} ({(train_pos / train_count) * 100:.2f}%)\")\n",
    "print(f\"Test minority: {test_pos:,} ({(test_pos / test_count) * 100:.2f}%)\")\n",
    "print(f\"Split time: {split_time:.1f}s\")\n",
    "\n",
    "# Bounded evaluation sample for local sklearn metrics\n",
    "eval_sample_rows = min(200_000, test_count)\n",
    "eval_test_df = test_ray_ds.limit(eval_sample_rows).to_pandas()\n",
    "X_test_eval = eval_test_df[feature_columns]\n",
    "y_test_eval = eval_test_df[\"label\"]\n",
    "print(f\"Evaluation sample rows: {len(eval_test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Training (Ray Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ray.train.xgboost import RayTrainReportCallback, XGBoostConfig\nfrom ray.train import ScalingConfig, RunConfig\nfrom ray.train.data_parallel_trainer import DataParallelTrainer\nimport ray.data\nimport ray.train\n\n# ======================================================================\n# OMP Diagnostics Collector — zero-CPU Ray actor that collects OMP state\n# from workers and makes it accessible to the driver for MLflow logging.\n# This solves the problem that worker print() statements go to Ray worker\n# stdout which is not accessible via the Databricks REST API.\n# ======================================================================\n@ray.remote(num_cpus=0)\nclass OmpDiagnosticsCollector:\n    \"\"\"Collects OMP diagnostic results from training workers.\"\"\"\n    def __init__(self):\n        self._results = {}\n\n    def report(self, worker_rank: int, diagnostics: dict):\n        \"\"\"Called by each worker to report its OMP state.\"\"\"\n        self._results[worker_rank] = diagnostics\n\n    def get_all(self) -> dict:\n        \"\"\"Returns all collected diagnostics keyed by worker rank.\"\"\"\n        return dict(self._results)\n\nprint(\"OmpDiagnosticsCollector actor defined.\")\n\nxgb_nthread = cpus_per_worker  # name it clearly for logging\n\nxgb_params = {\n    \"objective\": \"binary:logistic\",\n    \"tree_method\": \"hist\",\n    \"nthread\": xgb_nthread,\n    \"max_depth\": 6,\n    \"learning_rate\": 0.1,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"seed\": 42,\n    \"verbosity\": 1,\n}\n\nnum_boost_round = 100\n\nscaling_config = ScalingConfig(\n    num_workers=num_workers,\n    use_gpu=False,\n    resources_per_worker={\"CPU\": cpus_per_worker},\n)\n\nimport os\nray_storage_path = f\"/Volumes/{catalog}/{schema}/ray_results/\"\nos.makedirs(ray_storage_path, exist_ok=True)\nprint(f\"Storage directory created: {ray_storage_path}\")\n\nrun_config = RunConfig(\n    storage_path=ray_storage_path,\n    name=\"xgb_ray_plasma_tune\",\n)\n\n\ndef xgb_train_fn(config: dict):\n    \"\"\"Custom XGBoost training function with OMP_NUM_THREADS fix + diagnostics.\n\n    ROOT CAUSE: Databricks/Spark sets OMP_NUM_THREADS=1 on executors.\n    Ray on Spark workers inherit this. When xgboost is imported, it loads\n    libxgboost.so which initializes the OpenMP runtime. The OpenMP runtime\n    reads OMP_NUM_THREADS=1 at initialization and caps all thread pools to 1.\n    XGBoost's C++ layer then does: effective_threads = min(nthread, omp_get_max_threads())\n    = min(14, 1) = 1.\n\n    FIX: Set OMP_NUM_THREADS env var BEFORE importing xgboost, so the\n    OpenMP runtime initializes with the correct thread count. Also call\n    omp_set_num_threads() on all available OpenMP libraries as belt-and-suspenders.\n\n    DIAGNOSTICS: Reports OMP state to OmpDiagnosticsCollector actor so the\n    driver can log it as MLflow params (worker stdout isn't accessible via API).\n    \"\"\"\n    import os\n    import ctypes\n\n    nthread = config.get(\"nthread\", 1)\n    diag_collector_ref = config.get(\"_omp_diag_ref\")\n    worker_rank = ray.train.get_context().get_world_rank()\n    diagnostics = {}\n\n    # ===================================================================\n    # CRITICAL: Set OMP_NUM_THREADS BEFORE importing xgboost.\n    # The OpenMP runtime reads this env var at initialization time (when\n    # the first OpenMP-using shared library is loaded). Once initialized\n    # with threads=1, some runtimes ignore later omp_set_num_threads().\n    # ===================================================================\n    omp_env_before = os.environ.get(\"OMP_NUM_THREADS\", \"NOT_SET\")\n    os.environ[\"OMP_NUM_THREADS\"] = str(nthread)\n    diagnostics[\"omp_env_at_start\"] = omp_env_before\n    diagnostics[\"omp_env_set_to\"] = str(nthread)\n    print(f\"[Worker {worker_rank}] OMP_NUM_THREADS: was={omp_env_before}, now={os.environ['OMP_NUM_THREADS']}\")\n    print(f\"[Worker {worker_rank}] Requested nthread: {nthread}\")\n\n    # Also try to set via ctypes on ALL available OpenMP libraries BEFORE import\n    omp_libs_found = []\n    for lib_name in [\"libgomp.so.1\", \"libomp.so\", \"libomp.so.5\", \"libiomp5.so\"]:\n        try:\n            lib = ctypes.CDLL(lib_name)\n            lib.omp_get_max_threads.restype = ctypes.c_int\n            lib.omp_set_num_threads.argtypes = [ctypes.c_int]\n            omp_before = lib.omp_get_max_threads()\n            lib.omp_set_num_threads(nthread)\n            omp_after = lib.omp_get_max_threads()\n            omp_libs_found.append(f\"{lib_name}: {omp_before}->{omp_after}\")\n            diagnostics[f\"ctypes_{lib_name}\"] = f\"{omp_before}->{omp_after}\"\n            print(f\"[Worker {worker_rank}] {lib_name} omp_max_threads: {omp_before} -> {omp_after}\")\n        except OSError:\n            pass\n\n    if not omp_libs_found:\n        diagnostics[\"ctypes_libs\"] = \"NONE_FOUND\"\n        print(f\"[Worker {worker_rank}] WARNING: No OpenMP library found via ctypes\")\n    else:\n        diagnostics[\"ctypes_libs\"] = \";\".join(omp_libs_found)\n\n    # ===================================================================\n    # NOW import xgboost — the OpenMP runtime should initialize with\n    # OMP_NUM_THREADS already set to the correct value.\n    # ===================================================================\n    import xgboost\n\n    # Verify post-import: check that the OpenMP runtime has the right value\n    for lib_name in [\"libgomp.so.1\", \"libomp.so\", \"libomp.so.5\"]:\n        try:\n            lib = ctypes.CDLL(lib_name)\n            lib.omp_get_max_threads.restype = ctypes.c_int\n            actual = lib.omp_get_max_threads()\n            diagnostics[f\"post_import_{lib_name}\"] = str(actual)\n            print(f\"[Worker {worker_rank}] Post-import {lib_name} omp_max_threads: {actual}\")\n        except OSError:\n            pass\n\n    # Diagnostic: What OpenMP lib is XGBoost linked against?\n    xgb_omp_libs = []\n    try:\n        import subprocess\n        if hasattr(xgboost, '__file__'):\n            xgb_dir = os.path.dirname(xgboost.__file__)\n            ldd_result = subprocess.run(\n                [\"ldd\", os.path.join(xgb_dir, \"lib\", \"libxgboost.so\")],\n                capture_output=True, text=True, timeout=5\n            )\n            for line in ldd_result.stdout.split('\\n'):\n                if 'omp' in line.lower() or 'gomp' in line.lower():\n                    xgb_omp_libs.append(line.strip())\n                    print(f\"[Worker {worker_rank}] XGBoost linked OMP: {line.strip()}\")\n    except Exception as e:\n        print(f\"[Worker {worker_rank}] Could not inspect XGBoost libs: {e}\")\n    diagnostics[\"xgb_omp_linked_libs\"] = \"; \".join(xgb_omp_libs) if xgb_omp_libs else \"NONE_OR_ERROR\"\n\n    # Report diagnostics back to driver via collector actor\n    if diag_collector_ref is not None:\n        try:\n            ray.get(diag_collector_ref.report.remote(worker_rank, diagnostics), timeout=10)\n            print(f\"[Worker {worker_rank}] OMP diagnostics reported to collector\")\n        except Exception as e:\n            print(f\"[Worker {worker_rank}] Failed to report diagnostics: {e}\")\n\n    label_column = config[\"label_column\"]\n    num_boost_round = config[\"num_boost_round\"]\n    xgb_params = {k: v for k, v in config.items()\n                  if k not in (\"label_column\", \"num_boost_round\", \"dataset_keys\",\n                               \"_omp_diag_ref\")}\n\n    # Get dataset shards for this worker\n    train_ds = ray.train.get_dataset_shard(\"train\")\n    train_df = train_ds.materialize().to_pandas()\n    train_X = train_df.drop(label_column, axis=1)\n    train_y = train_df[label_column]\n    dtrain = xgboost.DMatrix(train_X, label=train_y)\n\n    evals = [(dtrain, \"train\")]\n\n    # Validation set if available\n    valid_ds = ray.train.get_dataset_shard(\"valid\")\n    if valid_ds is not None:\n        valid_df = valid_ds.materialize().to_pandas()\n        valid_X = valid_df.drop(label_column, axis=1)\n        valid_y = valid_df[label_column]\n        dvalid = xgboost.DMatrix(valid_X, label=valid_y)\n        evals.append((dvalid, \"valid\"))\n\n    # Resume from checkpoint if available\n    checkpoint = ray.train.get_checkpoint()\n    starting_model = None\n    remaining_iters = num_boost_round\n    if checkpoint:\n        starting_model = RayTrainReportCallback.get_model(checkpoint)\n        starting_iter = starting_model.num_boosted_rounds()\n        remaining_iters = num_boost_round - starting_iter\n\n    print(f\"[Worker {worker_rank}] Starting xgboost.train: nthread={xgb_params.get('nthread')}, \"\n          f\"boost_rounds={remaining_iters}, OMP_NUM_THREADS={os.environ.get('OMP_NUM_THREADS')}\")\n\n    # Train with RayTrainReportCallback for metrics reporting + checkpointing\n    bst = xgboost.train(\n        xgb_params,\n        dtrain=dtrain,\n        evals=evals,\n        num_boost_round=remaining_iters,\n        xgb_model=starting_model,\n        callbacks=[RayTrainReportCallback()],\n    )\n\n\n# Build config dict: xgb_params + metadata for the train function\ntrain_loop_config = {\n    **xgb_params,\n    \"label_column\": \"label\",\n    \"num_boost_round\": num_boost_round,\n}\n\nprint(\"XGBoost parameters:\")\nfor k, v in xgb_params.items():\n    print(f\"  {k}: {v}\")\nprint(f\"  num_boost_round: {num_boost_round}\")\nprint(f\"\\n=== OMP FIX STRATEGY ===\")\nprint(f\"1. runtime_env sets OMP_NUM_THREADS={xgb_nthread} at Ray worker process startup\")\nprint(f\"2. xgb_train_fn sets os.environ['OMP_NUM_THREADS']={xgb_nthread} BEFORE import xgboost\")\nprint(f\"3. ctypes omp_set_num_threads({xgb_nthread}) on all OMP libs BEFORE import xgboost\")\nprint(f\"4. xgb_params['nthread']={xgb_nthread} as the XGBoost-level parameter\")\nprint(f\"5. OmpDiagnosticsCollector reports worker OMP state back as MLflow params\")\nprint(f\"\\nRay ScalingConfig:\")\nprint(f\"  num_workers: {scaling_config.num_workers}\")\nprint(f\"  resources_per_worker: {scaling_config.resources_per_worker}\")\nprint(f\"\\nUsing DataParallelTrainer with custom train_loop_per_worker\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Train Ray Dataset: {train_count:,} rows\")\nprint(f\"Test Ray Dataset: {test_count:,} rows\")\n\n# Track worker monitor actors for cleanup\n_worker_monitor_actors = []\n\n# Create OMP diagnostics collector actor\n_omp_diag_collector = OmpDiagnosticsCollector.remote()\ntrain_loop_config[\"_omp_diag_ref\"] = _omp_diag_collector\nprint(\"OMP diagnostics collector created and passed to train config\")\n\nwith mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n    run_id = run.info.run_id\n    print(f\"MLflow run ID: {run_id}\")\n    print(f\"MLflow run name: {run_name}\")\n\n    # ======================================================================\n    # START WORKER-SIDE SYSTEM METRICS MONITORS\n    # Each worker node gets a SystemMetricsMonitor logging to the same run_id\n    # with a unique node_id prefix (e.g., system/worker_0/cpu_utilization_%)\n    # ======================================================================\n    try:\n        _worker_monitor_actors = start_worker_monitors(\n            run_id=run_id,\n            db_host=databricks_host_url,\n            db_token=databricks_token,\n            num_nodes=num_executors,\n            sampling_interval=10.0,\n        )\n        mlflow.log_param(\"worker_metrics_monitors\", len(_worker_monitor_actors))\n        print(f\"Worker metrics monitors active: {len(_worker_monitor_actors)}\")\n    except Exception as e:\n        import traceback\n        full_error = traceback.format_exc()\n        print(f\"WARNING: Could not start worker monitors: {e}\")\n        print(f\"Full traceback:\\n{full_error}\")\n        print(\"Continuing without worker-side system metrics.\")\n        mlflow.log_param(\"worker_metrics_monitors\", 0)\n        mlflow.log_param(\"worker_metrics_error\", str(e)[:500])\n    # ======================================================================\n\n    # Log input dataset\n    mlflow.log_input(mlflow_dataset, context=\"training\")\n    print(f\"Logged input dataset: {input_table}\")\n\n    # Log standard parameters\n    mlflow.log_param(\"training_mode\", \"ray_distributed_plasma_tune\")\n    mlflow.log_param(\"data_size\", data_size)\n    mlflow.log_param(\"node_type\", node_type)\n    mlflow.log_param(\"run_mode\", run_mode)\n    mlflow.log_param(\"warehouse_id\", warehouse_id)\n    mlflow.log_param(\"input_table\", input_table)\n    mlflow.log_param(\"n_rows\", n_rows)\n    mlflow.log_param(\"n_features\", len(feature_columns))\n    mlflow.log_param(\"minority_ratio\", round(minority_ratio, 4))\n    mlflow.log_param(\"train_size\", train_count)\n    mlflow.log_param(\"test_size\", test_count)\n    mlflow.log_param(\"eval_sample_rows\", len(eval_test_df))\n\n    # Log Ray params\n    mlflow.log_param(\"num_workers\", num_workers)\n    mlflow.log_param(\"cpus_per_worker\", cpus_per_worker)\n    mlflow.log_param(\"spark_executors\", num_executors)\n    mlflow.log_param(\"num_boost_round\", num_boost_round)\n\n    # === Log OMP fix strategy ===\n    mlflow.log_param(\"omp_fix_strategy\", \"runtime_env+env_before_import+ctypes+diag_collector\")\n    mlflow.log_param(\"omp_target_threads\", cpus_per_worker)\n\n    # === Log Plasma/Object Store config params ===\n    mlflow.log_param(\"plasma_obj_store_mem_gb\", obj_store_mem_gb)\n    mlflow.log_param(\"plasma_head_obj_store_mem_gb\", head_obj_store_mem_gb)\n    mlflow.log_param(\"plasma_heap_mem_gb\", heap_mem_gb)\n    mlflow.log_param(\"plasma_spill_dir\", spill_dir)\n    mlflow.log_param(\"plasma_ray_temp_dir\", ray_temp_dir)\n    mlflow.log_param(\"plasma_allow_slow_storage\", allow_slow_storage)\n    mlflow.log_param(\"plasma_tag\", plasma_tag)\n\n    # Log actual object store sizes from Ray nodes\n    try:\n        nodes_info = ray.nodes()\n        total_obj_store = sum(n.get('Resources', {}).get('object_store_memory', 0) for n in nodes_info if n.get('Alive'))\n        total_memory = sum(n.get('Resources', {}).get('memory', 0) for n in nodes_info if n.get('Alive'))\n        mlflow.log_metric(\"actual_total_obj_store_gb\", round(total_obj_store / (1024**3), 2))\n        mlflow.log_metric(\"actual_total_memory_gb\", round(total_memory / (1024**3), 2))\n        mlflow.log_metric(\"actual_node_count\", len([n for n in nodes_info if n.get('Alive')]))\n    except Exception:\n        pass\n\n    # Log XGBoost params\n    for k, v in xgb_params.items():\n        mlflow.log_param(f\"xgb_{k}\", v)\n\n    # Log timing\n    mlflow.log_metric(\"ray_init_time_sec\", ray_init_time)\n    mlflow.log_metric(\"data_load_time_sec\", load_time)\n    mlflow.log_metric(\"split_time_sec\", split_time)\n\n    # ======================================================================\n    # TRAINING (with worker monitor cleanup in finally block)\n    # ======================================================================\n    try:\n        # Train model\n        print(\"\\nTraining XGBoost with Ray Train (DataParallelTrainer + OMP fix)...\")\n        print(f\"OMP fix: runtime_env OMP_NUM_THREADS={cpus_per_worker} + env before import + ctypes\")\n        train_start = time.time()\n\n        trainer = DataParallelTrainer(\n            train_loop_per_worker=xgb_train_fn,\n            train_loop_config=train_loop_config,\n            scaling_config=scaling_config,\n            run_config=run_config,\n            datasets={\"train\": train_ray_ds, \"valid\": test_ray_ds},\n            backend_config=XGBoostConfig(),\n        )\n        result = trainer.fit()\n\n        train_time = time.time() - train_start\n        print(f\"Training completed in {train_time:.1f}s\")\n        print(f\"Best result: {result.metrics}\")\n\n        mlflow.log_metric(\"train_time_sec\", train_time)\n\n    finally:\n        # ======================================================================\n        # STOP WORKER-SIDE SYSTEM METRICS MONITORS\n        # Always stop monitors, even if training fails, to flush buffered metrics\n        # ======================================================================\n        stop_worker_monitors(_worker_monitor_actors)\n        _worker_monitor_actors = []\n\n    # ======================================================================\n    # COLLECT AND LOG OMP DIAGNOSTICS FROM WORKERS\n    # ======================================================================\n    try:\n        omp_diag_results = ray.get(_omp_diag_collector.get_all.remote(), timeout=15)\n        print(f\"\\n=== OMP DIAGNOSTICS FROM {len(omp_diag_results)} WORKERS ===\")\n        for rank in sorted(omp_diag_results.keys()):\n            diag = omp_diag_results[rank]\n            print(f\"\\nWorker {rank}:\")\n            for k, v in sorted(diag.items()):\n                print(f\"  {k}: {v}\")\n                # Log each diagnostic as an MLflow param\n                param_key = f\"omp_w{rank}_{k}\"\n                # MLflow param values are limited to 500 chars\n                mlflow.log_param(param_key, str(v)[:500])\n        mlflow.log_param(\"omp_diag_workers_reporting\", len(omp_diag_results))\n    except Exception as e:\n        print(f\"WARNING: Could not collect OMP diagnostics: {e}\")\n        mlflow.log_param(\"omp_diag_error\", str(e)[:500])\n    # ======================================================================\n\n    # Predictions on bounded evaluation sample\n    print(\"\\nGenerating predictions on evaluation sample...\")\n    pred_start = time.time()\n\n    import xgboost as xgb\n    checkpoint = result.checkpoint\n    booster = RayTrainReportCallback.get_model(checkpoint)\n\n    dtest = xgb.DMatrix(X_test_eval)\n    y_pred_proba = booster.predict(dtest)\n    y_pred = (y_pred_proba > 0.5).astype(int)\n\n    pred_time = time.time() - pred_start\n    mlflow.log_metric(\"predict_time_sec\", pred_time)\n\n    # Evaluation\n    print(\"\\nEvaluating...\")\n    from sklearn.metrics import (\n        average_precision_score,\n        roc_auc_score,\n        f1_score,\n        precision_score,\n        recall_score,\n        classification_report,\n        confusion_matrix,\n    )\n\n    auc_pr = average_precision_score(y_test_eval, y_pred_proba)\n    auc_roc = roc_auc_score(y_test_eval, y_pred_proba)\n    f1 = f1_score(y_test_eval, y_pred)\n    precision = precision_score(y_test_eval, y_pred, zero_division=0)\n    recall = recall_score(y_test_eval, y_pred, zero_division=0)\n\n    print(f\"\\nResults:\")\n    print(f\"  AUC-PR (primary): {auc_pr:.4f}\")\n    print(f\"  AUC-ROC: {auc_roc:.4f}\")\n    print(f\"  F1: {f1:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n\n    mlflow.log_metric(\"auc_pr\", auc_pr)\n    mlflow.log_metric(\"auc_roc\", auc_roc)\n    mlflow.log_metric(\"f1\", f1)\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n\n    cm = confusion_matrix(y_test_eval, y_pred)\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}\")\n    print(f\"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}\")\n\n    mlflow.log_metric(\"true_negatives\", cm[0, 0])\n    mlflow.log_metric(\"false_positives\", cm[0, 1])\n    mlflow.log_metric(\"false_negatives\", cm[1, 0])\n    mlflow.log_metric(\"true_positives\", cm[1, 1])\n\n    print(f\"\\nClassification Report:\")\n    print(classification_report(y_test_eval, y_pred, zero_division=0))\n\n    # Total time\n    total_time = ray_init_time + load_time + split_time + train_time + pred_time\n    mlflow.log_metric(\"total_time_sec\", total_time)\n\n    print(f\"\\n\" + \"=\"*60)\n    print(f\"Run complete: {run_name}\")\n    print(f\"Plasma config: {plasma_tag}\")\n    print(f\"OMP fix: runtime_env + env_before_import + ctypes (target={cpus_per_worker} threads)\")\n    print(f\"Total time: {total_time:.1f}s (Ray init: {ray_init_time:.1f}s, Load: {load_time:.1f}s, Train: {train_time:.1f}s)\")\n    print(f\"MLflow run ID: {run_id}\")\n    print(f\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shutting down Ray cluster...\")\n",
    "shutdown_ray_cluster()\n",
    "print(\"Ray cluster shutdown complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    result = {\n",
    "        \"status\": \"ok\" if not _notebook_errors else \"error\",\n",
    "        \"run_name\": run_name,\n",
    "        \"run_id\": run_id,\n",
    "        \"training_mode\": \"ray_distributed_plasma_tune\",\n",
    "        \"data_size\": data_size,\n",
    "        \"node_type\": node_type,\n",
    "        \"warehouse_id\": warehouse_id,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"cpus_per_worker\": cpus_per_worker,\n",
    "        \"spark_executors\": num_executors,\n",
    "        \"plasma_config\": {\n",
    "            \"obj_store_mem_gb\": obj_store_mem_gb,\n",
    "            \"head_obj_store_mem_gb\": head_obj_store_mem_gb,\n",
    "            \"heap_mem_gb\": heap_mem_gb,\n",
    "            \"spill_dir\": spill_dir,\n",
    "            \"ray_temp_dir\": ray_temp_dir,\n",
    "            \"allow_slow_storage\": allow_slow_storage,\n",
    "        },\n",
    "        \"auc_pr\": round(auc_pr, 4),\n",
    "        \"train_time_sec\": round(train_time, 1),\n",
    "        \"total_time_sec\": round(total_time, 1),\n",
    "    }\n",
    "    if _notebook_errors:\n",
    "        result[\"errors\"] = _notebook_errors\n",
    "except NameError as e:\n",
    "    result = {\n",
    "        \"status\": \"error\",\n",
    "        \"error\": f\"Notebook failed before completion: {e}\",\n",
    "        \"errors\": _notebook_errors if '_notebook_errors' in dir() else [],\n",
    "    }\n",
    "\n",
    "result_json = json.dumps(result)\n",
    "print(f\"\\nNotebook result: {result_json}\")\n",
    "\n",
    "dbutils.notebook.exit(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}