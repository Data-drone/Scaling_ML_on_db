{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost (Ray on Spark - Distributed)\n\nDistributed XGBoost training using Ray on Spark with MLflow tracking,\n**OMP_NUM_THREADS fix**, per-worker diagnostics, and system metrics collection.\n\n**Key fix:** Databricks silently sets `OMP_NUM_THREADS=1` on executors, causing\nXGBoost to use only 1 CPU core. This notebook applies a 3-layer fix:\n1. `spark.executorEnv.OMP_NUM_THREADS` in Spark config (Layer 1 - databricks.yml)\n2. `ray.init(runtime_env={\"env_vars\": {\"OMP_NUM_THREADS\": ...}})` (Layer 2)\n3. Worker-level `os.environ` + ctypes before `import xgboost` (Layer 3)\n\n**Features:**\n- OMP diagnostics via `OmpDiagnosticsCollector` actor (zero-CPU)\n- Per-worker system metrics via `WorkerMetricsMonitor` actors\n- `DataParallelTrainer` with custom train function for OMP control\n- Environment validation gate (`src.validate_env`)\n- Shared config presets from `src.config`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global error tracking\n_notebook_errors = []\ndef log_error(error_msg, exc=None):\n    import traceback\n    entry = {\"error\": str(error_msg)}\n    if exc: entry[\"traceback\"] = traceback.format_exc()\n    _notebook_errors.append(entry)\n    print(f\"ERROR LOGGED: {error_msg}\")\n\ndbutils.widgets.dropdown(\"data_size\", \"tiny\", [\"tiny\", \"small\", \"medium\", \"medium_large\", \"large\", \"xlarge\"], \"Data Size\")\ndbutils.widgets.text(\"node_type\", \"D8sv5\", \"Node Type\")\ndbutils.widgets.dropdown(\"run_mode\", \"full\", [\"full\", \"smoke\"], \"Run Mode\")\ndbutils.widgets.text(\"num_workers\", \"0\", \"Num Workers (0=auto)\")\ndbutils.widgets.text(\"cpus_per_worker\", \"0\", \"CPUs per Worker (0=auto)\")\ndbutils.widgets.text(\"warehouse_id\", \"148ccb90800933a1\", \"Databricks SQL Warehouse ID\")\ndbutils.widgets.text(\"catalog\", \"brian_gen_ai\", \"Catalog\")\ndbutils.widgets.text(\"schema\", \"xgb_scaling\", \"Schema\")\ndbutils.widgets.text(\"table_name\", \"\", \"Table Name (override)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get widget values\ndata_size = dbutils.widgets.get(\"data_size\")\nnode_type = dbutils.widgets.get(\"node_type\")\nrun_mode = dbutils.widgets.get(\"run_mode\")\nnum_workers_input = int(dbutils.widgets.get(\"num_workers\"))\ncpus_per_worker_input = int(dbutils.widgets.get(\"cpus_per_worker\"))\nwarehouse_id = dbutils.widgets.get(\"warehouse_id\").strip()\ncatalog = dbutils.widgets.get(\"catalog\")\nschema = dbutils.widgets.get(\"schema\")\ntable_name_override = dbutils.widgets.get(\"table_name\").strip()\n\nimport sys, os\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nrepo_root = \"/\".join(notebook_path.split(\"/\")[:-2])\nsys.path.insert(0, f\"/Workspace{repo_root}\")\nfrom src.config import PRESETS as CONFIG_PRESETS, get_preset\n\nSIZE_PRESETS = {n: {\"suffix\": p.table_suffix, \"rows\": p.rows, \"features\": p.total_features} for n, p in CONFIG_PRESETS.items()}\n\nif table_name_override:\n    input_table = f\"{catalog}.{schema}.{table_name_override}\"\n    data_size_label = table_name_override.replace(\"imbalanced_\", \"\")\nelse:\n    preset = get_preset(data_size)\n    input_table = f\"{catalog}.{schema}.imbalanced_{preset.table_suffix}\"\n    data_size_label = data_size\n\nrun_name = f\"ray_smoke_{node_type}\" if run_mode == \"smoke\" else f\"ray_{data_size_label}{'_'+str(num_workers_input)+'w' if num_workers_input > 0 else ''}_{node_type}\"\nprint(f\"Config: {data_size} | {node_type} | {run_mode} | table={input_table} | run={run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Validation Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validate_env import validate_environment\nvalidate_environment(track=\"ray-scaling\", expected_workers=num_workers_input if num_workers_input > 0 else None, raise_on_failure=False)\n",
    "if not _env_report.passed:\n",
    "    print(f\"WARNING: {len(_env_report.errors)} validation error(s) \u2014 continuing anyway for debugging\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Enable system metrics logging BEFORE importing mlflow\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Also call the enable function after import\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# Get current user for experiment path\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{user_email}/xgb_scaling_benchmark\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment: {experiment_path}\")\n",
    "print(f\"System metrics logging enabled: {os.environ.get('MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING')}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray on Spark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Check Ray version and availability\n",
    "try:\n",
    "    import ray\n",
    "    print(f\"Ray version: {ray.__version__}\")\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(f\"Ray not available: {e}\")\n",
    "\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# Setup Databricks environment for Ray workers (required for MLflow + Ray Data access)\n",
    "databricks_host_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "databricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "# Ray Data Databricks reader expects hostname without scheme.\n",
    "databricks_host = databricks_host_url.replace(\"https://\", \"\").replace(\"http://\", \"\").rstrip(\"/\")\n",
    "\n",
    "# Default env setup for MLflow and general Databricks SDK usage.\n",
    "os.environ[\"DATABRICKS_HOST\"] = databricks_host_url\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = databricks_token\n",
    "print(f\"Databricks Host URL: {databricks_host_url}\")\n",
    "print(f\"Databricks Hostname (Ray Data): {databricks_host}\")\n",
    "print(\"Databricks Token: [CONFIGURED]\")\n",
    "\n",
    "# Get cluster info\n",
    "sc = spark.sparkContext\n",
    "num_executors = sc._jsc.sc().getExecutorMemoryStatus().size() - 1  # Exclude driver\n",
    "print(f\"\\nSpark executors (workers): {num_executors}\")\n",
    "\n",
    "if num_executors < 1:\n",
    "    print(\"WARNING: No executors detected. This may indicate a single-node cluster.\")\n",
    "    print(\"Ray on Spark requires a multi-node cluster (num_workers >= 1)\")\n",
    "\n",
    "# Determine per-node vCPU and leave one CPU for Spark/system buffer\n",
    "import re\n",
    "node_type_lower = node_type.lower()\n",
    "node_vcpus_match = re.search(r\"[de](\\d+)\", node_type_lower)\n",
    "node_vcpus = int(node_vcpus_match.group(1)) if node_vcpus_match else 8\n",
    "allocatable_cpus_per_node = max(1, node_vcpus - 1)\n",
    "\n",
    "# Determine number of Ray workers (best practice: one worker per executor)\n",
    "if num_workers_input > 0:\n",
    "    num_workers = num_workers_input\n",
    "else:\n",
    "    num_workers = max(1, num_executors)\n",
    "\n",
    "# Keep worker count aligned with available executors\n",
    "if num_workers > num_executors:\n",
    "    print(f\"Capping num_workers from {num_workers} to {num_executors} (executor count)\")\n",
    "    num_workers = num_executors\n",
    "\n",
    "# CPUs per worker for Ray training\n",
    "if cpus_per_worker_input > 0:\n",
    "    cpus_per_worker = cpus_per_worker_input\n",
    "else:\n",
    "    cpus_per_worker = allocatable_cpus_per_node\n",
    "\n",
    "# Enforce per-node CPU safety cap\n",
    "if cpus_per_worker > allocatable_cpus_per_node:\n",
    "    print(\n",
    "        f\"Capping cpus_per_worker from {cpus_per_worker} to {allocatable_cpus_per_node} \"\n",
    "        f\"for node type {node_type}\"\n",
    "    )\n",
    "    cpus_per_worker = allocatable_cpus_per_node\n",
    "\n",
    "# Use the same per-node CPU for Ray cluster worker allocation\n",
    "num_cpus_worker_node = allocatable_cpus_per_node\n",
    "\n",
    "print(f\"\\nResource sizing:\")\n",
    "print(f\"  node_type: {node_type}\")\n",
    "print(f\"  node_vcpus: {node_vcpus}\")\n",
    "print(f\"  allocatable_cpus_per_node: {allocatable_cpus_per_node}\")\n",
    "print(f\"  spark_executors: {num_executors}\")\n",
    "print(f\"\\nRay training configuration:\")\n",
    "print(f\"  num_workers: {num_workers}\")\n",
    "print(f\"  cpus_per_worker: {cpus_per_worker}\")\n",
    "print(f\"  total_requested_cpus: {num_workers * cpus_per_worker}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray cluster + OMP fix\nprint(\"Starting Ray cluster...\")\nray_start = time.time()\nos.environ[\"OMP_NUM_THREADS\"] = str(allocatable_cpus_per_node)\n\ntry:\n    setup_ray_cluster(min_worker_nodes=num_executors, max_worker_nodes=num_executors,\n        num_cpus_worker_node=num_cpus_worker_node, num_gpus_worker_node=0, collect_log_to_path=\"/tmp/ray_logs\")\n    ray_init_time = time.time() - ray_start\n\n    # CRITICAL OMP FIX (Layer 2): Reconnect with runtime_env\n    omp_threads_str = str(cpus_per_worker)\n    if ray.is_initialized(): ray.shutdown()\n    ray.init(runtime_env={\"env_vars\": {\"OMP_NUM_THREADS\": omp_threads_str,\n        \"DATABRICKS_HOST\": databricks_host_url, \"DATABRICKS_TOKEN\": databricks_token}})\n    print(f\"Ray reconnected. OMP_NUM_THREADS={omp_threads_str}, init={ray_init_time:.1f}s\")\n\n    cluster_resources = ray.cluster_resources()\n    print(f\"Resources: {cluster_resources}\")\n    available_cpus = int(cluster_resources.get(\"CPU\", 0))\n    required = num_workers * cpus_per_worker + 1\n    if required > available_cpus:\n        usable = max(1, available_cpus - 1)\n        cpus_per_worker = max(1, usable // max(1, num_workers))\n        num_workers = max(1, usable // max(1, cpus_per_worker))\n    print(f\"Final: {num_workers}W x {cpus_per_worker}CPU = {num_workers*cpus_per_worker}+1 overhead\")\nexcept Exception as e:\n    import traceback; traceback.print_exc(); raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker-Side System Metrics & OMP Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n\n@ray.remote(num_cpus=0)\nclass WorkerMetricsMonitor:\n    def __init__(self, run_id, node_id, db_host, db_token, sampling_interval=10.0):\n        import os\n        os.environ.update({\"DATABRICKS_HOST\": db_host, \"DATABRICKS_TOKEN\": db_token, \"MLFLOW_TRACKING_URI\": \"databricks\"})\n        self._run_id, self._node_id, self._si, self._mon = run_id, node_id, sampling_interval, None\n        self._rn = ray.get_runtime_context().get_node_id()[:8]\n    def start(self):\n        from mlflow.system_metrics.system_metrics_monitor import SystemMetricsMonitor\n        self._mon = SystemMetricsMonitor(run_id=self._run_id, node_id=self._node_id, sampling_interval=self._si, samples_before_logging=1)\n        self._mon.start(); return f\"{self._node_id} on {self._rn}\"\n    def stop(self):\n        if self._mon: self._mon.finish(); self._mon = None; return f\"{self._node_id} stopped\"\n        return f\"{self._node_id} n/a\"\n\n@ray.remote(num_cpus=0)\nclass OmpDiagnosticsCollector:\n    def __init__(self): self._r = {}\n    def report(self, rank, diag): self._r[rank] = diag\n    def get_all(self): return dict(self._r)\n\ndef start_worker_monitors(run_id, db_host, db_token, num_nodes, si=10.0):\n    head = ray.get_runtime_context().get_node_id()\n    nodes = [n for n in ray.nodes() if n.get(\"Alive\") and n[\"NodeID\"] != head][:num_nodes]\n    actors, futs = [], []\n    for i, n in enumerate(nodes):\n        a = WorkerMetricsMonitor.options(scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(node_id=n[\"NodeID\"], soft=False), name=f\"metrics_w{i}\").remote(run_id, f\"worker_{i}\", db_host, db_token, si)\n        futs.append(a.start.remote()); actors.append(a)\n    for r in ray.get(futs): print(f\"  {r}\")\n    return actors\n\ndef stop_worker_monitors(actors):\n    if not actors: return\n    try:\n        for r in ray.get([a.stop.remote() for a in actors], timeout=30): print(f\"  {r}\")\n    except Exception as e: print(f\"  WARN: {e}\")\n    for a in actors:\n        try: ray.kill(a)\n        except: pass\n\nprint(\"WorkerMetricsMonitor + OmpDiagnosticsCollector defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import ray.data\n",
    "\n",
    "if not warehouse_id:\n",
    "    raise ValueError(\"warehouse_id is required for distributed Ray Data loading\")\n",
    "\n",
    "print(f\"Loading data from: {input_table}\")\n",
    "print(f\"Using SQL Warehouse: {warehouse_id}\")\n",
    "load_start = time.time()\n",
    "\n",
    "query = f\"SELECT * FROM {input_table}\"\n",
    "\n",
    "# Ray Data's Databricks reader expects DATABRICKS_HOST without scheme.\n",
    "_original_db_host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = databricks_host\n",
    "try:\n",
    "    full_ray_ds = ray.data.read_databricks_tables(\n",
    "        warehouse_id=warehouse_id,\n",
    "        query=query,\n",
    "    )\n",
    "finally:\n",
    "    # Restore default host URL for downstream APIs.\n",
    "    if _original_db_host is not None:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = _original_db_host\n",
    "    else:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = databricks_host_url\n",
    "\n",
    "n_rows = full_ray_ds.count()\n",
    "all_columns = list(full_ray_ds.schema().names)\n",
    "if \"label\" not in all_columns:\n",
    "    raise ValueError(f\"Expected 'label' column in dataset schema, got: {all_columns}\")\n",
    "\n",
    "feature_columns = [c for c in all_columns if c != \"label\"]\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "print(f\"Loaded {n_rows:,} rows x {len(all_columns)} columns in {load_time:.1f}s\")\n",
    "print(f\"Feature count: {len(feature_columns)}\")\n",
    "\n",
    "# Create a lightweight MLflow input dataset sample (avoid driver OOM)\n",
    "mlflow_sample_rows = min(10_000, n_rows)\n",
    "mlflow_sample_df = full_ray_ds.limit(mlflow_sample_rows).to_pandas()\n",
    "mlflow_dataset = mlflow.data.from_pandas(\n",
    "    mlflow_sample_df,\n",
    "    source=input_table,\n",
    "    name=data_size_label,\n",
    "    targets=\"label\",\n",
    ")\n",
    "print(f\"MLflow dataset sample created: {mlflow_dataset.name} ({mlflow_sample_rows:,} rows)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Class distribution from Ray Dataset\n",
    "positive_count = int(full_ray_ds.sum(\"label\"))\n",
    "negative_count = int(n_rows - positive_count)\n",
    "minority_ratio = positive_count / n_rows if n_rows else 0.0\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Class 0 (majority): {negative_count:,} ({(negative_count / n_rows) * 100:.2f}%)\")\n",
    "print(f\"  Class 1 (minority): {positive_count:,} ({(positive_count / n_rows) * 100:.2f}%)\")\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = negative_count / max(positive_count, 1)\n",
    "print(f\"\\nscale_pos_weight: {scale_pos_weight:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train/test split in Ray Data (keeps ingestion distributed)\n",
    "split_start = time.time()\n",
    "train_ray_ds, test_ray_ds = full_ray_ds.train_test_split(test_size=0.2, seed=42)\n",
    "split_time = time.time() - split_start\n",
    "\n",
    "train_count = train_ray_ds.count()\n",
    "test_count = test_ray_ds.count()\n",
    "train_pos = int(train_ray_ds.sum(\"label\"))\n",
    "test_pos = int(test_ray_ds.sum(\"label\"))\n",
    "\n",
    "print(f\"Train set: {train_count:,} rows\")\n",
    "print(f\"Test set: {test_count:,} rows\")\n",
    "print(f\"Train minority: {train_pos:,} ({(train_pos / train_count) * 100:.2f}%)\")\n",
    "print(f\"Test minority: {test_pos:,} ({(test_pos / test_count) * 100:.2f}%)\")\n",
    "print(f\"Split time: {split_time:.1f}s\")\n",
    "\n",
    "# Bounded evaluation sample for local sklearn metrics\n",
    "# This avoids collecting the full test split to driver memory.\n",
    "eval_sample_rows = min(200_000, test_count)\n",
    "eval_test_df = test_ray_ds.limit(eval_sample_rows).to_pandas()\n",
    "X_test_eval = eval_test_df[feature_columns]\n",
    "y_test_eval = eval_test_df[\"label\"]\n",
    "print(f\"Evaluation sample rows: {len(eval_test_df):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Training (Ray Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import RayTrainReportCallback, XGBoostConfig\nfrom ray.train import ScalingConfig, RunConfig\nfrom ray.train.data_parallel_trainer import DataParallelTrainer\nimport ray.data, ray.train\n\nxgb_params = {\"objective\": \"binary:logistic\", \"tree_method\": \"hist\", \"nthread\": cpus_per_worker,\n    \"max_depth\": 6, \"learning_rate\": 0.1, \"scale_pos_weight\": scale_pos_weight, \"seed\": 42, \"verbosity\": 1}\nnum_boost_round = 100\nscaling_config = ScalingConfig(num_workers=num_workers, use_gpu=False, resources_per_worker={\"CPU\": cpus_per_worker})\nray_storage_path = f\"/Volumes/{catalog}/{schema}/ray_results/\"\nos.makedirs(ray_storage_path, exist_ok=True)\nrun_config = RunConfig(storage_path=ray_storage_path, name=\"xgb_ray_train\")\n\ndef xgb_train_fn(config):\n    import os, ctypes\n    nthread, diag_ref = config.get(\"nthread\", 1), config.get(\"_omp_diag_ref\")\n    rank = ray.train.get_context().get_world_rank()\n    diag = {\"omp_before\": os.environ.get(\"OMP_NUM_THREADS\", \"NOT_SET\")}\n    os.environ[\"OMP_NUM_THREADS\"] = str(nthread)\n    diag[\"omp_set_to\"] = str(nthread)\n    for ln in [\"libgomp.so.1\", \"libomp.so\", \"libomp.so.5\"]:\n        try:\n            lib = ctypes.CDLL(ln); lib.omp_get_max_threads.restype = ctypes.c_int\n            lib.omp_set_num_threads.argtypes = [ctypes.c_int]\n            b = lib.omp_get_max_threads(); lib.omp_set_num_threads(nthread)\n            diag[f\"ctypes_{ln}\"] = f\"{b}->{lib.omp_get_max_threads()}\"\n        except OSError: pass\n    import xgboost\n    for ln in [\"libgomp.so.1\"]:\n        try:\n            lib = ctypes.CDLL(ln); lib.omp_get_max_threads.restype = ctypes.c_int\n            diag[f\"post_{ln}\"] = str(lib.omp_get_max_threads())\n        except: pass\n    if diag_ref:\n        try: ray.get(diag_ref.report.remote(rank, diag), timeout=10)\n        except: pass\n    label_col, n_rounds = config[\"label_column\"], config[\"num_boost_round\"]\n    xp = {k: v for k, v in config.items() if k not in (\"label_column\",\"num_boost_round\",\"dataset_keys\",\"_omp_diag_ref\")}\n    tdf = ray.train.get_dataset_shard(\"train\").materialize().to_pandas()\n    dtrain = xgboost.DMatrix(tdf.drop(label_col, axis=1), label=tdf[label_col])\n    evals = [(dtrain, \"train\")]\n    vds = ray.train.get_dataset_shard(\"valid\")\n    if vds:\n        vdf = vds.materialize().to_pandas()\n        evals.append((xgboost.DMatrix(vdf.drop(label_col, axis=1), label=vdf[label_col]), \"valid\"))\n    ckpt = ray.train.get_checkpoint()\n    sm, iters = None, n_rounds\n    if ckpt: sm = RayTrainReportCallback.get_model(ckpt); iters = n_rounds - sm.num_boosted_rounds()\n    xgboost.train(xp, dtrain, evals=evals, num_boost_round=iters, xgb_model=sm, callbacks=[RayTrainReportCallback()])\n\ntrain_loop_config = {**xgb_params, \"label_column\": \"label\", \"num_boost_round\": num_boost_round}\nprint(f\"XGB: {xgb_params} | rounds={num_boost_round}\")\nprint(f\"OMP: spark_conf(L1) + runtime_env(L2) + env+ctypes before import(L3)\")\nprint(f\"DataParallelTrainer: {num_workers}W x {cpus_per_worker}CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train: {train_count:,} | Test: {test_count:,}\")\n_mons, _omp = [], OmpDiagnosticsCollector.remote()\ntrain_loop_config[\"_omp_diag_ref\"] = _omp\n\nwith mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n    run_id = run.info.run_id\n    print(f\"MLflow: {run_id} ({run_name})\")\n    try:\n        _mons = start_worker_monitors(run_id, databricks_host_url, databricks_token, num_executors)\n        mlflow.log_param(\"worker_metrics_monitors\", len(_mons))\n    except Exception as e:\n        print(f\"WARN: monitors failed: {e}\"); mlflow.log_param(\"worker_metrics_monitors\", 0)\n\n    mlflow.log_input(mlflow_dataset, context=\"training\")\n    for k, v in {\"training_mode\": \"ray_distributed\", \"data_size\": data_size, \"node_type\": node_type,\n        \"run_mode\": run_mode, \"input_table\": input_table, \"n_rows\": n_rows,\n        \"n_features\": len(feature_columns), \"num_workers\": num_workers,\n        \"cpus_per_worker\": cpus_per_worker, \"num_boost_round\": num_boost_round,\n        \"omp_fix\": \"spark_conf+runtime_env+env_before_import+ctypes+diag\",\n        \"omp_target\": cpus_per_worker}.items():\n        mlflow.log_param(k, v)\n    for k, v in xgb_params.items(): mlflow.log_param(f\"xgb_{k}\", v)\n    mlflow.log_metric(\"ray_init_time_sec\", ray_init_time)\n    mlflow.log_metric(\"data_load_time_sec\", load_time)\n    mlflow.log_metric(\"split_time_sec\", split_time)\n\n    try:\n        print(\"\\nTraining with DataParallelTrainer + OMP fix...\")\n        t0 = time.time()\n        trainer = DataParallelTrainer(train_loop_per_worker=xgb_train_fn, train_loop_config=train_loop_config,\n            scaling_config=scaling_config, run_config=run_config,\n            datasets={\"train\": train_ray_ds, \"valid\": test_ray_ds}, backend_config=XGBoostConfig())\n        result = trainer.fit()\n        train_time = time.time() - t0\n        print(f\"Done in {train_time:.1f}s\"); mlflow.log_metric(\"train_time_sec\", train_time)\n    finally:\n        stop_worker_monitors(_mons); _mons = []\n\n    try:\n        od = ray.get(_omp.get_all.remote(), timeout=15)\n        print(f\"\\nOMP diag ({len(od)} workers):\")\n        for r in sorted(od):\n            for k, v in sorted(od[r].items()): print(f\"  w{r}/{k}: {v}\"); mlflow.log_param(f\"omp_w{r}_{k}\", str(v)[:500])\n    except Exception as e: print(f\"WARN: OMP diag failed: {e}\")\n\n    import xgboost as xgb\n    t0 = time.time()\n    booster = RayTrainReportCallback.get_model(result.checkpoint)\n    yp = booster.predict(xgb.DMatrix(X_test_eval))\n    y_pred = (yp > 0.5).astype(int)\n    pred_time = time.time() - t0; mlflow.log_metric(\"predict_time_sec\", pred_time)\n\n    from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n    auc_pr, auc_roc = average_precision_score(y_test_eval, yp), roc_auc_score(y_test_eval, yp)\n    f1 = f1_score(y_test_eval, y_pred)\n    prec, rec = precision_score(y_test_eval, y_pred, zero_division=0), recall_score(y_test_eval, y_pred, zero_division=0)\n    for n, v in [(\"auc_pr\",auc_pr),(\"auc_roc\",auc_roc),(\"f1\",f1),(\"precision\",prec),(\"recall\",rec)]:\n        mlflow.log_metric(n, v); print(f\"  {n}: {v:.4f}\")\n    cm = confusion_matrix(y_test_eval, y_pred)\n    for n, v in [(\"true_negatives\",cm[0,0]),(\"false_positives\",cm[0,1]),(\"false_negatives\",cm[1,0]),(\"true_positives\",cm[1,1])]:\n        mlflow.log_metric(n, v)\n    print(classification_report(y_test_eval, y_pred, zero_division=0))\n    total_time = ray_init_time + load_time + split_time + train_time + pred_time\n    mlflow.log_metric(\"total_time_sec\", total_time)\n    print(f\"\\nDone: {run_name} | {total_time:.1f}s | OMP={cpus_per_worker} | {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shutting down Ray cluster...\")\n",
    "shutdown_ray_cluster()\n",
    "print(\"Ray cluster shutdown complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Check if we have all expected variables (indicates successful run)\n",
    "try:\n",
    "    result = {\n",
    "        \"status\": \"ok\" if not _notebook_errors else \"error\",\n",
    "        \"run_name\": run_name,\n",
    "        \"run_id\": run_id,\n",
    "        \"training_mode\": \"ray_distributed\",\n",
    "        \"data_size\": data_size,\n",
    "        \"node_type\": node_type,\n",
    "        \"warehouse_id\": warehouse_id,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"cpus_per_worker\": cpus_per_worker,\n",
    "        \"spark_executors\": num_executors,\n",
    "        \"auc_pr\": round(auc_pr, 4),\n",
    "        \"train_time_sec\": round(train_time, 1),\n",
    "        \"total_time_sec\": round(total_time, 1),\n",
    "    }\n",
    "    if _notebook_errors:\n",
    "        result[\"errors\"] = _notebook_errors\n",
    "except NameError as e:\n",
    "    # Some variables weren't defined - notebook failed early\n",
    "    result = {\n",
    "        \"status\": \"error\",\n",
    "        \"error\": f\"Notebook failed before completion: {e}\",\n",
    "        \"errors\": _notebook_errors if '_notebook_errors' in dir() else [],\n",
    "    }\n",
    "\n",
    "result_json = json.dumps(result)\n",
    "print(f\"\\nNotebook result: {result_json}\")\n",
    "\n",
    "dbutils.notebook.exit(result_json)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
