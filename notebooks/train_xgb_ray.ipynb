{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost (Ray on Spark - Distributed)\n",
    "\n",
    "Distributed XGBoost training using Ray on Spark with MLflow tracking.\n",
    "\n",
    "**Requirements:**\n",
    "- Databricks ML Runtime 17.3 LTS (includes Ray)\n",
    "- Multi-node cluster (2+ workers recommended)\n",
    "\n",
    "**Parameters:**\n",
    "- `data_size`: Dataset size preset (tiny/small/medium/large/xlarge)\n",
    "- `node_type`: Node type for run naming (e.g., D8sv5)\n",
    "- `run_mode`: `full` or `smoke` (smoke uses tiny data)\n",
    "- `num_workers`: Number of Ray workers (0 = auto based on cluster)\n",
    "- `cpus_per_worker`: CPUs allocated per Ray worker (0 = auto)\n",
    "- `warehouse_id`: Databricks SQL Warehouse ID for Ray distributed data loading\n",
    "\n",
    "**MLflow:**\n",
    "- System metrics enabled\n",
    "- Run name: `ray_{data_size}_{node_type}` or `ray_smoke_{node_type}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Widgets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Global error tracking - captures errors from any cell\n",
    "_notebook_errors = []\n",
    "\n",
    "def log_error(error_msg, exc=None):\n",
    "    \"\"\"Log an error for later retrieval in exit cell.\"\"\"\n",
    "    import traceback\n",
    "    entry = {\"error\": str(error_msg)}\n",
    "    if exc:\n",
    "        entry[\"traceback\"] = traceback.format_exc()\n",
    "    _notebook_errors.append(entry)\n",
    "    print(f\"ERROR LOGGED: {error_msg}\")\n",
    "\n",
    "dbutils.widgets.dropdown(\"data_size\", \"tiny\", [\"tiny\", \"small\", \"medium\", \"large\", \"xlarge\"], \"Data Size\")\n",
    "dbutils.widgets.text(\"node_type\", \"D8sv5\", \"Node Type\")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"full\", [\"full\", \"smoke\"], \"Run Mode\")\n",
    "dbutils.widgets.text(\"num_workers\", \"0\", \"Num Workers (0=auto)\")\n",
    "dbutils.widgets.text(\"cpus_per_worker\", \"0\", \"CPUs per Worker (0=auto)\")\n",
    "dbutils.widgets.text(\"warehouse_id\", \"148ccb90800933a1\", \"Databricks SQL Warehouse ID\")\n",
    "dbutils.widgets.text(\"catalog\", \"brian_gen_ai\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"xgb_scaling\", \"Schema\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Table Name (override)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get widget values\n",
    "data_size = dbutils.widgets.get(\"data_size\")\n",
    "node_type = dbutils.widgets.get(\"node_type\")\n",
    "run_mode = dbutils.widgets.get(\"run_mode\")\n",
    "num_workers_input = int(dbutils.widgets.get(\"num_workers\"))\n",
    "cpus_per_worker_input = int(dbutils.widgets.get(\"cpus_per_worker\"))\n",
    "warehouse_id = dbutils.widgets.get(\"warehouse_id\").strip()\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name_override = dbutils.widgets.get(\"table_name\").strip()\n",
    "\n",
    "# Dataset size preset mapping\n",
    "SIZE_PRESETS = {\n",
    "    \"tiny\": {\"suffix\": \"10k\", \"rows\": 10_000, \"features\": 20},\n",
    "    \"small\": {\"suffix\": \"1m\", \"rows\": 1_000_000, \"features\": 100},\n",
    "    \"medium\": {\"suffix\": \"10m\", \"rows\": 10_000_000, \"features\": 250},\n",
    "    \"large\": {\"suffix\": \"100m\", \"rows\": 100_000_000, \"features\": 500},\n",
    "    \"xlarge\": {\"suffix\": \"500m\", \"rows\": 500_000_000, \"features\": 500},\n",
    "}\n",
    "\n",
    "# Determine input table\n",
    "if table_name_override:\n",
    "    input_table = f\"{catalog}.{schema}.{table_name_override}\"\n",
    "    data_size_label = table_name_override.replace(\"imbalanced_\", \"\")\n",
    "else:\n",
    "    preset = SIZE_PRESETS[data_size]\n",
    "    table_suffix = preset[\"suffix\"]\n",
    "    input_table = f\"{catalog}.{schema}.imbalanced_{table_suffix}\"\n",
    "    data_size_label = data_size\n",
    "\n",
    "# Run naming (prefix with ray_ and worker config)\n",
    "if run_mode == \"smoke\":\n",
    "    run_name = f\"ray_smoke_{node_type}\"\n",
    "else:\n",
    "    worker_suffix = f\"_{num_workers_input}w\" if num_workers_input > 0 else \"\"\n",
    "    run_name = f\"ray_{data_size_label}{worker_suffix}_{node_type}\"\n",
    "\n",
    "print(f\"Data size: {data_size}\")\n",
    "print(f\"Node type: {node_type}\")\n",
    "print(f\"Run mode: {run_mode}\")\n",
    "print(f\"Num workers input: {num_workers_input} (0=auto)\")\n",
    "print(f\"CPUs per worker input: {cpus_per_worker_input} (0=auto)\")\n",
    "print(f\"Warehouse ID: {warehouse_id}\")\n",
    "print(f\"Input table: {input_table}\")\n",
    "print(f\"Run name: {run_name}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Enable system metrics logging BEFORE importing mlflow\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Also call the enable function after import\n",
    "mlflow.enable_system_metrics_logging()\n",
    "\n",
    "# Get current user for experiment path\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{user_email}/xgb_scaling_benchmark\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"MLflow experiment: {experiment_path}\")\n",
    "print(f\"System metrics logging enabled: {os.environ.get('MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING')}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray on Spark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "# Check Ray version and availability\n",
    "try:\n",
    "    import ray\n",
    "    print(f\"Ray version: {ray.__version__}\")\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(f\"Ray not available: {e}\")\n",
    "\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# Setup Databricks environment for Ray workers (required for MLflow + Ray Data access)\n",
    "databricks_host_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "databricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "\n",
    "# Ray Data Databricks reader expects hostname without scheme.\n",
    "databricks_host = databricks_host_url.replace(\"https://\", \"\").replace(\"http://\", \"\").rstrip(\"/\")\n",
    "\n",
    "# Default env setup for MLflow and general Databricks SDK usage.\n",
    "os.environ[\"DATABRICKS_HOST\"] = databricks_host_url\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = databricks_token\n",
    "print(f\"Databricks Host URL: {databricks_host_url}\")\n",
    "print(f\"Databricks Hostname (Ray Data): {databricks_host}\")\n",
    "print(\"Databricks Token: [CONFIGURED]\")\n",
    "\n",
    "# Get cluster info\n",
    "sc = spark.sparkContext\n",
    "num_executors = sc._jsc.sc().getExecutorMemoryStatus().size() - 1  # Exclude driver\n",
    "print(f\"\\nSpark executors (workers): {num_executors}\")\n",
    "\n",
    "if num_executors < 1:\n",
    "    print(\"WARNING: No executors detected. This may indicate a single-node cluster.\")\n",
    "    print(\"Ray on Spark requires a multi-node cluster (num_workers >= 1)\")\n",
    "\n",
    "# Determine per-node vCPU and leave one CPU for Spark/system buffer\n",
    "import re\n",
    "node_type_lower = node_type.lower()\n",
    "node_vcpus_match = re.search(r\"[de](\\d+)\", node_type_lower)\n",
    "node_vcpus = int(node_vcpus_match.group(1)) if node_vcpus_match else 8\n",
    "allocatable_cpus_per_node = max(1, node_vcpus - 1)\n",
    "\n",
    "# Determine number of Ray workers (best practice: one worker per executor)\n",
    "if num_workers_input > 0:\n",
    "    num_workers = num_workers_input\n",
    "else:\n",
    "    num_workers = max(1, num_executors)\n",
    "\n",
    "# Keep worker count aligned with available executors\n",
    "if num_workers > num_executors:\n",
    "    print(f\"Capping num_workers from {num_workers} to {num_executors} (executor count)\")\n",
    "    num_workers = num_executors\n",
    "\n",
    "# CPUs per worker for Ray training\n",
    "if cpus_per_worker_input > 0:\n",
    "    cpus_per_worker = cpus_per_worker_input\n",
    "else:\n",
    "    cpus_per_worker = allocatable_cpus_per_node\n",
    "\n",
    "# Enforce per-node CPU safety cap\n",
    "if cpus_per_worker > allocatable_cpus_per_node:\n",
    "    print(\n",
    "        f\"Capping cpus_per_worker from {cpus_per_worker} to {allocatable_cpus_per_node} \"\n",
    "        f\"for node type {node_type}\"\n",
    "    )\n",
    "    cpus_per_worker = allocatable_cpus_per_node\n",
    "\n",
    "# Use the same per-node CPU for Ray cluster worker allocation\n",
    "num_cpus_worker_node = allocatable_cpus_per_node\n",
    "\n",
    "print(f\"\\nResource sizing:\")\n",
    "print(f\"  node_type: {node_type}\")\n",
    "print(f\"  node_vcpus: {node_vcpus}\")\n",
    "print(f\"  allocatable_cpus_per_node: {allocatable_cpus_per_node}\")\n",
    "print(f\"  spark_executors: {num_executors}\")\n",
    "print(f\"\\nRay training configuration:\")\n",
    "print(f\"  num_workers: {num_workers}\")\n",
    "print(f\"  cpus_per_worker: {cpus_per_worker}\")\n",
    "print(f\"  total_requested_cpus: {num_workers * cpus_per_worker}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Ray cluster on Spark\n",
    "print(\"Starting Ray cluster on Spark...\")\n",
    "ray_start = time.time()\n",
    "\n",
    "try:\n",
    "    # Ray on Spark setup with per-node CPU safety buffer\n",
    "    ray_cluster = setup_ray_cluster(\n",
    "        min_worker_nodes=num_executors,         # Fixed size cluster for benchmarks\n",
    "        max_worker_nodes=num_executors,         # No autoscaling for benchmarks\n",
    "        num_cpus_worker_node=num_cpus_worker_node,\n",
    "        num_gpus_worker_node=0,                 # CPU-only training\n",
    "        collect_log_to_path=\"/tmp/ray_logs\"\n",
    "    )\n",
    "    ray_init_time = time.time() - ray_start\n",
    "    print(f\"Ray cluster initialized in {ray_init_time:.1f}s\")\n",
    "    print(f\"Ray head node: {ray.get_runtime_context().node_id}\")\n",
    "    \n",
    "    print(f\"\\nRay cluster resources:\")\n",
    "    cluster_resources = ray.cluster_resources()\n",
    "    print(cluster_resources)\n",
    "\n",
    "    # Preflight CPU validation to avoid trainer stalls from pending actors\n",
    "    AUTO_CAP_RESOURCES = True\n",
    "    # Ray Train needs coordinator/driver CPU in addition to worker CPUs.\n",
    "    ray_train_overhead_cpus = 1\n",
    "    available_cpus = int(cluster_resources.get(\"CPU\", 0))\n",
    "    required_worker_cpus = int(num_workers * cpus_per_worker)\n",
    "    required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n",
    "\n",
    "    print(\"\\nRay preflight CPU check:\")\n",
    "    print(f\"  available_cpus: {available_cpus}\")\n",
    "    print(f\"  requested_worker_cpus: {required_worker_cpus} ({num_workers} workers x {cpus_per_worker} CPU)\")\n",
    "    print(f\"  ray_train_overhead_cpus: {ray_train_overhead_cpus}\")\n",
    "    print(f\"  requested_total_cpus: {required_total_cpus}\")\n",
    "\n",
    "    if required_total_cpus > available_cpus:\n",
    "        if not AUTO_CAP_RESOURCES:\n",
    "            raise RuntimeError(\n",
    "                f\"Insufficient Ray CPUs: requested total {required_total_cpus}, available {available_cpus}. \"\n",
    "                \"Reduce num_workers/cpus_per_worker or increase cluster size.\"\n",
    "            )\n",
    "\n",
    "        # Cap CPUs per worker first, then worker count if still needed.\n",
    "        usable_cpus_for_workers = max(1, available_cpus - ray_train_overhead_cpus)\n",
    "        capped_cpus_per_worker = max(1, usable_cpus_for_workers // max(1, num_workers))\n",
    "        if capped_cpus_per_worker < cpus_per_worker:\n",
    "            print(\n",
    "                f\"  auto-cap: cpus_per_worker {cpus_per_worker} -> {capped_cpus_per_worker}\"\n",
    "            )\n",
    "            cpus_per_worker = capped_cpus_per_worker\n",
    "\n",
    "        required_worker_cpus = int(num_workers * cpus_per_worker)\n",
    "        required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n",
    "        if required_total_cpus > available_cpus:\n",
    "            capped_workers = max(1, usable_cpus_for_workers // max(1, cpus_per_worker))\n",
    "            if capped_workers < num_workers:\n",
    "                print(f\"  auto-cap: num_workers {num_workers} -> {capped_workers}\")\n",
    "                num_workers = capped_workers\n",
    "\n",
    "        required_worker_cpus = int(num_workers * cpus_per_worker)\n",
    "        required_total_cpus = required_worker_cpus + ray_train_overhead_cpus\n",
    "        if required_total_cpus > available_cpus:\n",
    "            raise RuntimeError(\n",
    "                f\"Unable to fit Ray resources after auto-cap: requested total {required_total_cpus}, available {available_cpus}.\"\n",
    "            )\n",
    "\n",
    "    print(f\"  final_num_workers: {num_workers}\")\n",
    "    print(f\"  final_cpus_per_worker: {cpus_per_worker}\")\n",
    "    print(f\"  final_requested_worker_cpus: {num_workers * cpus_per_worker}\")\n",
    "    print(f\"  final_requested_total_cpus: {(num_workers * cpus_per_worker) + ray_train_overhead_cpus}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Ray cluster: {e}\")\n",
    "    print(f\"\\nDebug info:\")\n",
    "    print(f\"  Spark version: {spark.version}\")\n",
    "    print(f\"  Executors: {num_executors}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import ray.data\n",
    "\n",
    "if not warehouse_id:\n",
    "    raise ValueError(\"warehouse_id is required for distributed Ray Data loading\")\n",
    "\n",
    "print(f\"Loading data from: {input_table}\")\n",
    "print(f\"Using SQL Warehouse: {warehouse_id}\")\n",
    "load_start = time.time()\n",
    "\n",
    "query = f\"SELECT * FROM {input_table}\"\n",
    "\n",
    "# Ray Data's Databricks reader expects DATABRICKS_HOST without scheme.\n",
    "_original_db_host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = databricks_host\n",
    "try:\n",
    "    full_ray_ds = ray.data.read_databricks_tables(\n",
    "        warehouse_id=warehouse_id,\n",
    "        query=query,\n",
    "    )\n",
    "finally:\n",
    "    # Restore default host URL for downstream APIs.\n",
    "    if _original_db_host is not None:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = _original_db_host\n",
    "    else:\n",
    "        os.environ[\"DATABRICKS_HOST\"] = databricks_host_url\n",
    "\n",
    "n_rows = full_ray_ds.count()\n",
    "all_columns = list(full_ray_ds.schema().names)\n",
    "if \"label\" not in all_columns:\n",
    "    raise ValueError(f\"Expected 'label' column in dataset schema, got: {all_columns}\")\n",
    "\n",
    "feature_columns = [c for c in all_columns if c != \"label\"]\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "print(f\"Loaded {n_rows:,} rows x {len(all_columns)} columns in {load_time:.1f}s\")\n",
    "print(f\"Feature count: {len(feature_columns)}\")\n",
    "\n",
    "# Create a lightweight MLflow input dataset sample (avoid driver OOM)\n",
    "mlflow_sample_rows = min(10_000, n_rows)\n",
    "mlflow_sample_df = full_ray_ds.limit(mlflow_sample_rows).to_pandas()\n",
    "mlflow_dataset = mlflow.data.from_pandas(\n",
    "    mlflow_sample_df,\n",
    "    source=input_table,\n",
    "    name=data_size_label,\n",
    "    targets=\"label\",\n",
    ")\n",
    "print(f\"MLflow dataset sample created: {mlflow_dataset.name} ({mlflow_sample_rows:,} rows)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Class distribution from Ray Dataset\n",
    "positive_count = int(full_ray_ds.sum(\"label\"))\n",
    "negative_count = int(n_rows - positive_count)\n",
    "minority_ratio = positive_count / n_rows if n_rows else 0.0\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Class 0 (majority): {negative_count:,} ({(negative_count / n_rows) * 100:.2f}%)\")\n",
    "print(f\"  Class 1 (minority): {positive_count:,} ({(positive_count / n_rows) * 100:.2f}%)\")\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale_pos_weight = negative_count / max(positive_count, 1)\n",
    "print(f\"\\nscale_pos_weight: {scale_pos_weight:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train/test split in Ray Data (keeps ingestion distributed)\n",
    "split_start = time.time()\n",
    "train_ray_ds, test_ray_ds = full_ray_ds.train_test_split(test_size=0.2, seed=42)\n",
    "split_time = time.time() - split_start\n",
    "\n",
    "train_count = train_ray_ds.count()\n",
    "test_count = test_ray_ds.count()\n",
    "train_pos = int(train_ray_ds.sum(\"label\"))\n",
    "test_pos = int(test_ray_ds.sum(\"label\"))\n",
    "\n",
    "print(f\"Train set: {train_count:,} rows\")\n",
    "print(f\"Test set: {test_count:,} rows\")\n",
    "print(f\"Train minority: {train_pos:,} ({(train_pos / train_count) * 100:.2f}%)\")\n",
    "print(f\"Test minority: {test_pos:,} ({(test_pos / test_count) * 100:.2f}%)\")\n",
    "print(f\"Split time: {split_time:.1f}s\")\n",
    "\n",
    "# Bounded evaluation sample for local sklearn metrics\n",
    "# This avoids collecting the full test split to driver memory.\n",
    "eval_sample_rows = min(200_000, test_count)\n",
    "eval_test_df = test_ray_ds.limit(eval_sample_rows).to_pandas()\n",
    "X_test_eval = eval_test_df[feature_columns]\n",
    "y_test_eval = eval_test_df[\"label\"]\n",
    "print(f\"Evaluation sample rows: {len(eval_test_df):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Training (Ray Distributed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Ray Train's XGBoostTrainer (modern approach, replaces deprecated xgboost_ray)\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "import ray.data\n",
    "\n",
    "# XGBoost hyperparameters - same as single-node for comparison\n",
    "# Note: Uses xgboost.train() style params, not sklearn API\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"nthread\": cpus_per_worker,  # Must be explicit \u2014 Ray Train does NOT auto-set this\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"seed\": 42,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "# Number of boosting rounds\n",
    "num_boost_round = 100\n",
    "\n",
    "# Ray scaling config\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=False,\n",
    "    resources_per_worker={\"CPU\": cpus_per_worker},\n",
    ")\n",
    "\n",
    "# Storage path for Ray Train checkpoints\n",
    "# Using local SSD storage available on each Databricks node\n",
    "import os\n",
    "\n",
    "# use volume path for shared storage folder\n",
    "ray_storage_path = f\"/Volumes/{catalog}/{schema}/ray_results/\"\n",
    "\n",
    "# Ensure the storage directory exists\n",
    "os.makedirs(ray_storage_path, exist_ok=True)\n",
    "print(f\"Storage directory created: {ray_storage_path}\")\n",
    "\n",
    "# Run config with local storage path\n",
    "run_config = RunConfig(\n",
    "    storage_path=ray_storage_path,\n",
    "    name=\"xgb_ray_train\",\n",
    ")\n",
    "\n",
    "print(f\"Ray RunConfig:\")\n",
    "print(f\"  storage_path: {ray_storage_path}\")\n",
    "\n",
    "print(\"XGBoost parameters:\")\n",
    "for k, v in xgb_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"  num_boost_round: {num_boost_round}\")\n",
    "\n",
    "print(f\"\\nRay ScalingConfig:\")\n",
    "print(f\"  num_workers: {scaling_config.num_workers}\")\n",
    "print(f\"  resources_per_worker: {scaling_config.resources_per_worker}\")\n",
    "\n",
    "print(f\"\\nRay RunConfig:\")\n",
    "print(f\"  storage_path: {ray_storage_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Train Ray Dataset: {train_count:,} rows\")\n",
    "print(f\"Test Ray Dataset: {test_count:,} rows\")\n",
    "\n",
    "# Start MLflow run and train\n",
    "with mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow run ID: {run_id}\")\n",
    "    print(f\"MLflow run name: {run_name}\")\n",
    "\n",
    "    # Log input dataset\n",
    "    mlflow.log_input(mlflow_dataset, context=\"training\")\n",
    "    print(f\"Logged input dataset: {input_table}\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"training_mode\", \"ray_distributed\")\n",
    "    mlflow.log_param(\"data_size\", data_size)\n",
    "    mlflow.log_param(\"node_type\", node_type)\n",
    "    mlflow.log_param(\"run_mode\", run_mode)\n",
    "    mlflow.log_param(\"warehouse_id\", warehouse_id)\n",
    "    mlflow.log_param(\"input_table\", input_table)\n",
    "    mlflow.log_param(\"n_rows\", n_rows)\n",
    "    mlflow.log_param(\"n_features\", len(feature_columns))\n",
    "    mlflow.log_param(\"minority_ratio\", round(minority_ratio, 4))\n",
    "    mlflow.log_param(\"train_size\", train_count)\n",
    "    mlflow.log_param(\"test_size\", test_count)\n",
    "    mlflow.log_param(\"eval_sample_rows\", len(eval_test_df))\n",
    "\n",
    "    # Log Ray params\n",
    "    mlflow.log_param(\"num_workers\", num_workers)\n",
    "    mlflow.log_param(\"cpus_per_worker\", cpus_per_worker)\n",
    "    mlflow.log_param(\"spark_executors\", num_executors)\n",
    "    mlflow.log_param(\"num_boost_round\", num_boost_round)\n",
    "\n",
    "    # Log XGBoost params\n",
    "    for k, v in xgb_params.items():\n",
    "        mlflow.log_param(f\"xgb_{k}\", v)\n",
    "\n",
    "    # Log timing\n",
    "    mlflow.log_metric(\"ray_init_time_sec\", ray_init_time)\n",
    "    mlflow.log_metric(\"data_load_time_sec\", load_time)\n",
    "    mlflow.log_metric(\"split_time_sec\", split_time)\n",
    "\n",
    "    # Train model using Ray Train XGBoostTrainer\n",
    "    print(\"\\nTraining XGBoost with Ray Train...\")\n",
    "    train_start = time.time()\n",
    "\n",
    "    trainer = XGBoostTrainer(\n",
    "        scaling_config=scaling_config,\n",
    "        run_config=run_config,\n",
    "        label_column=\"label\",\n",
    "        num_boost_round=num_boost_round,\n",
    "        params=xgb_params,\n",
    "        datasets={\"train\": train_ray_ds, \"valid\": test_ray_ds},\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "\n",
    "    train_time = time.time() - train_start\n",
    "    print(f\"Training completed in {train_time:.1f}s\")\n",
    "    print(f\"Best result: {result.metrics}\")\n",
    "\n",
    "    mlflow.log_metric(\"train_time_sec\", train_time)\n",
    "\n",
    "    # Get the trained model for bounded local evaluation\n",
    "    print(\"\\nGenerating predictions on evaluation sample...\")\n",
    "    pred_start = time.time()\n",
    "\n",
    "    # Load trained booster from checkpoint\n",
    "    import xgboost as xgb\n",
    "    checkpoint = result.checkpoint\n",
    "    with checkpoint.as_directory() as checkpoint_dir:\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(f\"{checkpoint_dir}/model.ubj\")\n",
    "\n",
    "    # Make predictions on bounded evaluation sample\n",
    "    dtest = xgb.DMatrix(X_test_eval)\n",
    "    y_pred_proba = booster.predict(dtest)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    pred_time = time.time() - pred_start\n",
    "    mlflow.log_metric(\"predict_time_sec\", pred_time)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nEvaluating...\")\n",
    "    from sklearn.metrics import (\n",
    "        average_precision_score,\n",
    "        roc_auc_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        classification_report,\n",
    "        confusion_matrix,\n",
    "    )\n",
    "\n",
    "    # Metrics on bounded evaluation sample\n",
    "    auc_pr = average_precision_score(y_test_eval, y_pred_proba)\n",
    "    auc_roc = roc_auc_score(y_test_eval, y_pred_proba)\n",
    "    f1 = f1_score(y_test_eval, y_pred)\n",
    "    precision = precision_score(y_test_eval, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test_eval, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  AUC-PR (primary): {auc_pr:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"auc_pr\", auc_pr)\n",
    "    mlflow.log_metric(\"auc_roc\", auc_roc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_eval, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}\")\n",
    "    print(f\"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}\")\n",
    "\n",
    "    mlflow.log_metric(\"true_negatives\", cm[0, 0])\n",
    "    mlflow.log_metric(\"false_positives\", cm[0, 1])\n",
    "    mlflow.log_metric(\"false_negatives\", cm[1, 0])\n",
    "    mlflow.log_metric(\"true_positives\", cm[1, 1])\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_eval, y_pred, zero_division=0))\n",
    "\n",
    "    # Total time\n",
    "    total_time = ray_init_time + load_time + split_time + train_time + pred_time\n",
    "    mlflow.log_metric(\"total_time_sec\", total_time)\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"Run complete: {run_name}\")\n",
    "    print(f\"Total time: {total_time:.1f}s (Ray init: {ray_init_time:.1f}s, Load: {load_time:.1f}s, Train: {train_time:.1f}s)\")\n",
    "    print(f\"MLflow run ID: {run_id}\")\n",
    "    print(f\"=\"*50)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray Cluster"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shutting down Ray cluster...\")\n",
    "shutdown_ray_cluster()\n",
    "print(\"Ray cluster shutdown complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Check if we have all expected variables (indicates successful run)\n",
    "try:\n",
    "    result = {\n",
    "        \"status\": \"ok\" if not _notebook_errors else \"error\",\n",
    "        \"run_name\": run_name,\n",
    "        \"run_id\": run_id,\n",
    "        \"training_mode\": \"ray_distributed\",\n",
    "        \"data_size\": data_size,\n",
    "        \"node_type\": node_type,\n",
    "        \"warehouse_id\": warehouse_id,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"cpus_per_worker\": cpus_per_worker,\n",
    "        \"spark_executors\": num_executors,\n",
    "        \"auc_pr\": round(auc_pr, 4),\n",
    "        \"train_time_sec\": round(train_time, 1),\n",
    "        \"total_time_sec\": round(total_time, 1),\n",
    "    }\n",
    "    if _notebook_errors:\n",
    "        result[\"errors\"] = _notebook_errors\n",
    "except NameError as e:\n",
    "    # Some variables weren't defined - notebook failed early\n",
    "    result = {\n",
    "        \"status\": \"error\",\n",
    "        \"error\": f\"Notebook failed before completion: {e}\",\n",
    "        \"errors\": _notebook_errors if '_notebook_errors' in dir() else [],\n",
    "    }\n",
    "\n",
    "result_json = json.dumps(result)\n",
    "print(f\"\\nNotebook result: {result_json}\")\n",
    "\n",
    "dbutils.notebook.exit(result_json)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}